{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c2367b",
   "metadata": {},
   "source": [
    "### 1: How to iterate over rows in a DataFrame in Pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08291f21",
   "metadata": {},
   "source": [
    "<p>I have a pandas dataframe, <code>df</code>:</p><pre><code>   c1   c20  10  1001  11  1102  12  120</code></pre><p>How do I iterate over the rows of this dataframe? For every row, I want to be able to access its elements (values in cells) by the name of the columns. For example:</p><pre><code>for row in df.rows:   print(row[c1], row[c2])</code></pre><hr /><p>I found a <a href=https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas>similar question</a> which suggests using either of these:</p><pre><code>for date, row in df.T.iteritems():</code></pre><pre><code>for row in df.iterrows():</code></pre><p>But I do not understand what the <code>row</code> object is and how I can work with it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to iterate over rows in a DataFrame in Pandas\"\n",
    "    __QUESTION = \"<p>I have a pandas dataframe, <code>df</code>:</p><pre><code>   c1   c20  10  1001  11  1102  12  120</code></pre><p>How do I iterate over the rows of this dataframe? For every row, I want to be able to access its elements (values in cells) by the name of the columns. For example:</p><pre><code>for row in df.rows:   print(row[c1], row[c2])</code></pre><hr /><p>I found a <a href=https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas>similar question</a> which suggests using either of these:</p><pre><code>for date, row in df.T.iteritems():</code></pre><pre><code>for row in df.iterrows():</code></pre><p>But I do not understand what the <code>row</code> object is and how I can work with it.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65ed2b",
   "metadata": {},
   "source": [
    "### 2: How do I select rows from a DataFrame based on column values?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87892095",
   "metadata": {},
   "source": [
    "<p>How can I select rows from a <code>DataFrame</code> based on values in some column in Pandas?</p><p>In SQL, I would use:</p><pre class=lang-sql prettyprint-override><code>SELECT *FROM tableWHERE column_name = some_value</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How do I select rows from a DataFrame based on column values?\"\n",
    "    __QUESTION = \"<p>How can I select rows from a <code>DataFrame</code> based on values in some column in Pandas?</p><p>In SQL, I would use:</p><pre class=lang-sql prettyprint-override><code>SELECT *FROM tableWHERE column_name = some_value</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66ce6d",
   "metadata": {},
   "source": [
    "### 3: Renaming column names in Pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c53e2",
   "metadata": {},
   "source": [
    "<p>How do I change the column labels of a pandas DataFrame from:</p><pre><code>[$a, $b, $c, $d, $e]</code></pre><p>to</p><pre><code>[a, b, c, d, e].</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14669165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'replace', 'dataframe', 'rename']\n",
    "    __TITLE = \"Renaming column names in Pandas\"\n",
    "    __QUESTION = \"<p>How do I change the column labels of a pandas DataFrame from:</p><pre><code>[$a, $b, $c, $d, $e]</code></pre><p>to</p><pre><code>[a, b, c, d, e].</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11346283/renaming-column-names-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cbc22",
   "metadata": {},
   "source": [
    "### 4: Delete a column from a Pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d473a",
   "metadata": {},
   "source": [
    "<p>To delete a column in a DataFrame, I can successfully use:</p><pre><code>del df[column_name]</code></pre><p>But why cant I use the following?</p><pre><code>del df.column_name</code></pre><p>Since it is possible to access the column/Series as <code>df.column_name</code>, I expected this to work.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Delete a column from a Pandas DataFrame\"\n",
    "    __QUESTION = \"<p>To delete a column in a DataFrame, I can successfully use:</p><pre><code>del df[column_name]</code></pre><p>But why cant I use the following?</p><pre><code>del df.column_name</code></pre><p>Since it is possible to access the column/Series as <code>df.column_name</code>, I expected this to work.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13411544/delete-a-column-from-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4723df",
   "metadata": {},
   "source": [
    "### 5: How do I get the row count of a Pandas DataFrame?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025ddb2",
   "metadata": {},
   "source": [
    "<p>How do I get the number of rows of a pandas dataframe <code>df</code>?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How do I get the row count of a Pandas DataFrame?\"\n",
    "    __QUESTION = \"<p>How do I get the number of rows of a pandas dataframe <code>df</code>?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f24fcb0",
   "metadata": {},
   "source": [
    "### 6: Selecting multiple columns in a Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f7b2d",
   "metadata": {},
   "source": [
    "<p>How do I select columns <code>a</code> and <code>b</code> from <code>df</code>, and save them into a new dataframe <code>df1</code>?</p><pre class=lang-none prettyprint-override><code>index  a   b   c1      2   3   42      3   4   5</code></pre><p>Unsuccessful attempt:</p><pre><code>df1 = df[a:b]df1 = df.ix[:, a:b]</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ac72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'select']\n",
    "    __TITLE = \"Selecting multiple columns in a Pandas dataframe\"\n",
    "    __QUESTION = \"<p>How do I select columns <code>a</code> and <code>b</code> from <code>df</code>, and save them into a new dataframe <code>df1</code>?</p><pre class=lang-none prettyprint-override><code>index  a   b   c1      2   3   42      3   4   5</code></pre><p>Unsuccessful attempt:</p><pre><code>df1 = df[a:b]df1 = df.ix[:, a:b]</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11285613/selecting-multiple-columns-in-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0877b",
   "metadata": {},
   "source": [
    "### 7: How to change the order of DataFrame columns?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef6d60",
   "metadata": {},
   "source": [
    "<p>I have the following <code>DataFrame</code> (<code>df</code>):</p><pre><code>import numpy as npimport pandas as pddf = pd.DataFrame(np.random.rand(10, 5))</code></pre><p>I add more column(s) by assignment:</p><pre><code>df[mean] = df.mean(1)</code></pre><p>How can I move the column <code>mean</code> to the front, i.e. set it as first column leaving the order of the other columns untouched?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to change the order of DataFrame columns?\"\n",
    "    __QUESTION = \"<p>I have the following <code>DataFrame</code> (<code>df</code>):</p><pre><code>import numpy as npimport pandas as pddf = pd.DataFrame(np.random.rand(10, 5))</code></pre><p>I add more column(s) by assignment:</p><pre><code>df[mean] = df.mean(1)</code></pre><p>How can I move the column <code>mean</code> to the front, i.e. set it as first column leaving the order of the other columns untouched?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5068401",
   "metadata": {},
   "source": [
    "### 8: Change column type in pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3184180",
   "metadata": {},
   "source": [
    "<p>I want to convert a table, represented as a list of lists, into a pandas DataFrame. As an extremely simplified example:</p><pre><code>a = [[a, 1.2, 4.2], [b, 70, 0.03], [x, 5, 0]]df = pd.DataFrame(a)</code></pre><p>What is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I dont want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'types', 'casting']\n",
    "    __TITLE = \"Change column type in pandas\"\n",
    "    __QUESTION = \"<p>I want to convert a table, represented as a list of lists, into a pandas DataFrame. As an extremely simplified example:</p><pre><code>a = [[a, 1.2, 4.2], [b, 70, 0.03], [x, 5, 0]]df = pd.DataFrame(a)</code></pre><p>What is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I dont want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/15891038/change-column-type-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ab747",
   "metadata": {},
   "source": [
    "### 9: Get a list from Pandas DataFrame column headers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eb70b",
   "metadata": {},
   "source": [
    "<p>I want to get a list of the column headers from a Pandas DataFrame.  The DataFrame will come from user input, so I wont know how many columns there will be or what they will be called.</p><p>For example, if Im given a DataFrame like this:</p><pre><code>&gt;&gt;&gt; my_dataframe    y  gdp  cap0   1    2    51   2    3    92   8    7    23   3    4    74   6    7    75   4    8    36   8    2    87   9    9   108   6    6    49  10   10    7</code></pre><p>I would get a list like this:</p><pre><code>&gt;&gt;&gt; header_list[y, gdp, cap]</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Get a list from Pandas DataFrame column headers\"\n",
    "    __QUESTION = \"<p>I want to get a list of the column headers from a Pandas DataFrame.  The DataFrame will come from user input, so I wont know how many columns there will be or what they will be called.</p><p>For example, if Im given a DataFrame like this:</p><pre><code>&gt;&gt;&gt; my_dataframe    y  gdp  cap0   1    2    51   2    3    92   8    7    23   3    4    74   6    7    75   4    8    36   8    2    87   9    9   108   6    6    49  10   10    7</code></pre><p>I would get a list like this:</p><pre><code>&gt;&gt;&gt; header_list[y, gdp, cap]</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19482970/get-a-list-from-pandas-dataframe-column-headers\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff09d4c",
   "metadata": {},
   "source": [
    "### 10: Create a Pandas Dataframe by appending one row at a time\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac5bdb",
   "metadata": {},
   "source": [
    "<p>How do I create an empty <code>DataFrame</code>, then add rows, one by one?</p><p>I created an empty <code>DataFrame</code>:</p><pre><code>df = pd.DataFrame(columns=(lib, qty1, qty2))</code></pre><p>Then I can add a new row at the end and fill a single field with:</p><pre><code>df = df._set_value(index=len(df), col=qty1, value=10.0)</code></pre><p>It works for only one field at a time. What is a better way to add new row to <code>df</code>?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f531fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'append']\n",
    "    __TITLE = \"Create a Pandas Dataframe by appending one row at a time\"\n",
    "    __QUESTION = \"<p>How do I create an empty <code>DataFrame</code>, then add rows, one by one?</p><p>I created an empty <code>DataFrame</code>:</p><pre><code>df = pd.DataFrame(columns=(lib, qty1, qty2))</code></pre><p>Then I can add a new row at the end and fill a single field with:</p><pre><code>df = df._set_value(index=len(df), col=qty1, value=10.0)</code></pre><p>It works for only one field at a time. What is a better way to add new row to <code>df</code>?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84f58d",
   "metadata": {},
   "source": [
    "### 11: How to add a new column to an existing DataFrame?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9f5b5",
   "metadata": {},
   "source": [
    "<p>I have the following indexed DataFrame with named columns and rows not- continuous numbers:</p><pre><code>          a         b         c         d2  0.671399  0.101208 -0.181532  0.2412733  0.446172 -0.243316  0.051767  1.5773185  0.614758  0.075793 -0.451460 -0.012493</code></pre><p>I would like to add a new column, <code>e</code>, to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). </p><pre><code>0   -0.3354851   -1.1666582   -0.385571dtype: float64</code></pre><p>How can I add column <code>e</code> to the above example? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'chained-assignment']\n",
    "    __TITLE = \"How to add a new column to an existing DataFrame?\"\n",
    "    __QUESTION = \"<p>I have the following indexed DataFrame with named columns and rows not- continuous numbers:</p><pre><code>          a         b         c         d2  0.671399  0.101208 -0.181532  0.2412733  0.446172 -0.243316  0.051767  1.5773185  0.614758  0.075793 -0.451460 -0.012493</code></pre><p>I would like to add a new column, <code>e</code>, to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). </p><pre><code>0   -0.3354851   -1.1666582   -0.385571dtype: float64</code></pre><p>How can I add column <code>e</code> to the above example? </p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/12555323/how-to-add-a-new-column-to-an-existing-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79923f3d",
   "metadata": {},
   "source": [
    "### 12: How to drop rows of Pandas DataFrame whose value in a certain column is NaN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded412b",
   "metadata": {},
   "source": [
    "<p>I have this <code>DataFrame</code> and want only the records whose <code>EPS</code> column is not <code>NaN</code>:</p><pre><code>&gt;&gt;&gt; df                 STK_ID  EPS  cashSTK_ID RPT_Date                   601166 20111231  601166  NaN   NaN600036 20111231  600036  NaN    12600016 20111231  600016  4.3   NaN601009 20111231  601009  NaN   NaN601939 20111231  601939  2.5   NaN000001 20111231  000001  NaN   NaN</code></pre><p>...i.e. something like <code>df.drop(....)</code> to get this resulting dataframe:</p><pre><code>                  STK_ID  EPS  cashSTK_ID RPT_Date                   600016 20111231  600016  4.3   NaN601939 20111231  601939  2.5   NaN</code></pre><p>How do I do that?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'nan']\n",
    "    __TITLE = \"How to drop rows of Pandas DataFrame whose value in a certain column is NaN\"\n",
    "    __QUESTION = \"<p>I have this <code>DataFrame</code> and want only the records whose <code>EPS</code> column is not <code>NaN</code>:</p><pre><code>&gt;&gt;&gt; df                 STK_ID  EPS  cashSTK_ID RPT_Date                   601166 20111231  601166  NaN   NaN600036 20111231  600036  NaN    12600016 20111231  600016  4.3   NaN601009 20111231  601009  NaN   NaN601939 20111231  601939  2.5   NaN000001 20111231  000001  NaN   NaN</code></pre><p>...i.e. something like <code>df.drop(....)</code> to get this resulting dataframe:</p><pre><code>                  STK_ID  EPS  cashSTK_ID RPT_Date                   600016 20111231  600016  4.3   NaN601939 20111231  601939  2.5   NaN</code></pre><p>How do I do that?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad44848",
   "metadata": {},
   "source": [
    "### 13: How to deal with SettingWithCopyWarning in Pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f886e6",
   "metadata": {},
   "source": [
    "<h2>Background</h2><p>I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:</p><pre><code>E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE</code></pre><p>I want to know what exactly it means?  Do I need to change something?</p><p>How should I suspend the warning if I insist to use <code>quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE</code>?</p><h2>The function that gives errors</h2><pre><code>def _decode_stock_quote(list_of_150_stk_str):    &quot;&quot;&quot;decode the webpage and return dataframe&quot;&quot;&quot;    from cStringIO import StringIO    str_of_all = &quot;&quot;.join(list_of_150_stk_str)    quote_df = pd.read_csv(StringIO(str_of_all), sep=,, names=list(ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg)) #dtype={A: object, B: object, C: np.float64}    quote_df.rename(columns={A:STK, B:TOpen, C:TPCLOSE, D:TPrice, E:THigh, F:TLow, I:TVol, J:TAmt, e:TDate, f:TTime}, inplace=True)    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]    quote_df[TClose] = quote_df[TPrice]    quote_df[RT]     = 100 * (quote_df[TPrice]/quote_df[TPCLOSE] - 1)    quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE    quote_df[TAmt]   = quote_df[TAmt]/TAMT_SCALE    quote_df[STK_ID] = quote_df[STK].str.slice(13,19)    quote_df[STK_Name] = quote_df[STK].str.slice(21,30)#.decode(gb2312)    quote_df[TDate]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])        return quote_df</code></pre><h2>More error messages</h2><pre><code>E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TVol]   = quote_df[TVol]/TVOL_SCALEE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TAmt]   = quote_df[TAmt]/TAMT_SCALEE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TDate]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'chained-assignment']\n",
    "    __TITLE = \"How to deal with SettingWithCopyWarning in Pandas\"\n",
    "    __QUESTION = \"<h2>Background</h2><p>I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:</p><pre><code>E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE</code></pre><p>I want to know what exactly it means?  Do I need to change something?</p><p>How should I suspend the warning if I insist to use <code>quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE</code>?</p><h2>The function that gives errors</h2><pre><code>def _decode_stock_quote(list_of_150_stk_str):    &quot;&quot;&quot;decode the webpage and return dataframe&quot;&quot;&quot;    from cStringIO import StringIO    str_of_all = &quot;&quot;.join(list_of_150_stk_str)    quote_df = pd.read_csv(StringIO(str_of_all), sep=,, names=list(ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg)) #dtype={A: object, B: object, C: np.float64}    quote_df.rename(columns={A:STK, B:TOpen, C:TPCLOSE, D:TPrice, E:THigh, F:TLow, I:TVol, J:TAmt, e:TDate, f:TTime}, inplace=True)    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]    quote_df[TClose] = quote_df[TPrice]    quote_df[RT]     = 100 * (quote_df[TPrice]/quote_df[TPCLOSE] - 1)    quote_df[TVol]   = quote_df[TVol]/TVOL_SCALE    quote_df[TAmt]   = quote_df[TAmt]/TAMT_SCALE    quote_df[STK_ID] = quote_df[STK].str.slice(13,19)    quote_df[STK_Name] = quote_df[STK].str.slice(21,30)#.decode(gb2312)    quote_df[TDate]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])        return quote_df</code></pre><h2>More error messages</h2><pre><code>E:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TVol]   = quote_df[TVol]/TVOL_SCALEE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TAmt]   = quote_df[TAmt]/TAMT_SCALEE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_index,col_indexer] = value instead  quote_df[TDate]  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c32ea",
   "metadata": {},
   "source": [
    "### 14: &quot;Large data&quot; workflows using pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6800d",
   "metadata": {},
   "source": [
    "<p>I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for its out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.</p><p>One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  Im not talking about big data that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.</p><p>My first thought is to use <code>HDFStore</code> to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:</p><p>What are some best-practice workflows for accomplishing the following:</p><ol><li>Loading flat files into a permanent, on-disk database structure</li><li>Querying that database to retrieve data to feed into a pandas data structure</li><li>Updating the database after manipulating pieces in pandas</li></ol><p>Real-world examples would be much appreciated, especially from anyone who uses pandas on large data.</p><p>Edit -- an example of how I would like this to work:</p><ol><li>Iteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.</li><li>In order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.</li><li>I would create new columns by performing various operations on the selected columns.</li><li>I would then have to append these new columns into the database structure.</li></ol><p>I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.</p><p>Edit -- Responding to Jeffs questions specifically:</p><ol><li>I am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.</li><li>Typical operations involve combining several columns using conditional logic into a new, compound column. For example, <code>if var1 &gt; 2 then newvar = A elif var2 = 4 then newvar = B</code>.  The result of these operations is a new column for every record in my dataset.</li><li>Finally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.</li><li>A typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.</li><li>Its pretty rare that I would subset by rows when creating a new column.  However, its pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.</li><li>The modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What Im doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.</li></ol><p>It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'mongodb', 'pandas', 'hdf5', 'large-data']\n",
    "    __TITLE = \"&quot;Large data&quot; workflows using pandas\"\n",
    "    __QUESTION = \"<p>I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for its out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.</p><p>One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  Im not talking about big data that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.</p><p>My first thought is to use <code>HDFStore</code> to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:</p><p>What are some best-practice workflows for accomplishing the following:</p><ol><li>Loading flat files into a permanent, on-disk database structure</li><li>Querying that database to retrieve data to feed into a pandas data structure</li><li>Updating the database after manipulating pieces in pandas</li></ol><p>Real-world examples would be much appreciated, especially from anyone who uses pandas on large data.</p><p>Edit -- an example of how I would like this to work:</p><ol><li>Iteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.</li><li>In order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.</li><li>I would create new columns by performing various operations on the selected columns.</li><li>I would then have to append these new columns into the database structure.</li></ol><p>I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.</p><p>Edit -- Responding to Jeffs questions specifically:</p><ol><li>I am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.</li><li>Typical operations involve combining several columns using conditional logic into a new, compound column. For example, <code>if var1 &gt; 2 then newvar = A elif var2 = 4 then newvar = B</code>.  The result of these operations is a new column for every record in my dataset.</li><li>Finally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.</li><li>A typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.</li><li>Its pretty rare that I would subset by rows when creating a new column.  However, its pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.</li><li>The modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What Im doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.</li></ol><p>It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/14262433/large-data-workflows-using-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ce70",
   "metadata": {},
   "source": [
    "### 15: Use a list of values to select rows from a Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2a5a3",
   "metadata": {},
   "source": [
    "<p>Let’s say I have the following Pandas dataframe:</p><pre><code>df = DataFrame({A : [5,6,3,4], B : [1,2,3, 5]})df     A   B0    5   11    6   22    3   33    4   5</code></pre><p>I can subset based on a specific value:</p><pre><code>x = df[df[A] == 3]x     A   B2    3   3</code></pre><p>But how can I subset based on a list of values? - something like this:</p><pre><code>list_of_values = [3,6]y = df[df[A] in list_of_values]</code></pre><p>To get:</p><pre><code>     A    B1    6    22    3    3</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8166fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Use a list of values to select rows from a Pandas dataframe\"\n",
    "    __QUESTION = \"<p>Let’s say I have the following Pandas dataframe:</p><pre><code>df = DataFrame({A : [5,6,3,4], B : [1,2,3, 5]})df     A   B0    5   11    6   22    3   33    4   5</code></pre><p>I can subset based on a specific value:</p><pre><code>x = df[df[A] == 3]x     A   B2    3   3</code></pre><p>But how can I subset based on a list of values? - something like this:</p><pre><code>list_of_values = [3,6]y = df[df[A] in list_of_values]</code></pre><p>To get:</p><pre><code>     A    B1    6    22    3    3</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb09d4",
   "metadata": {},
   "source": [
    "### 16: Pretty-print an entire Pandas Series / DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c6453",
   "metadata": {},
   "source": [
    "<p>I work with Series and DataFrames on the terminal a lot. The default <code>__repr__</code> for a Series returns a reduced sample, with some head and tail values, but the rest missing.</p><p>Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Pretty-print an entire Pandas Series / DataFrame\"\n",
    "    __QUESTION = \"<p>I work with Series and DataFrames on the terminal a lot. The default <code>__repr__</code> for a Series returns a reduced sample, with some head and tail values, but the rest missing.</p><p>Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95674831",
   "metadata": {},
   "source": [
    "### 17: Convert list of dictionaries to a pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fd3f0",
   "metadata": {},
   "source": [
    "<p>How can I convert a list of dictionaries into a <code>DataFrame</code>? Given:</p><pre><code>[{points: 50, time: 5:00, year: 2010},  {points: 25, time: 6:00, month: &quot;february&quot;},  {points:90, time: 9:00, month: january},  {points_h1:20, month: june}]</code></pre><p>I want to turn the above into a <code>DataFrame</code>:</p><pre><code>      month  points  points_h1  time  year0       NaN      50        NaN  5:00  20101  february      25        NaN  6:00   NaN2   january      90        NaN  9:00   NaN3      june     NaN         20   NaN   NaN</code></pre><p>Note: Order of the columns does not matter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'dictionary', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Convert list of dictionaries to a pandas DataFrame\"\n",
    "    __QUESTION = \"<p>How can I convert a list of dictionaries into a <code>DataFrame</code>? Given:</p><pre><code>[{points: 50, time: 5:00, year: 2010},  {points: 25, time: 6:00, month: &quot;february&quot;},  {points:90, time: 9:00, month: january},  {points_h1:20, month: june}]</code></pre><p>I want to turn the above into a <code>DataFrame</code>:</p><pre><code>      month  points  points_h1  time  year0       NaN      50        NaN  5:00  20101  february      25        NaN  6:00   NaN2   january      90        NaN  9:00   NaN3      june     NaN         20   NaN   NaN</code></pre><p>Note: Order of the columns does not matter.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f6579",
   "metadata": {},
   "source": [
    "### 18: Writing a pandas DataFrame to CSV file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2069a54",
   "metadata": {},
   "source": [
    "<p>I have a dataframe in pandas which I would like to write to a CSV file.</p><p>I am doing this using:</p><pre><code>df.to_csv(out.csv)</code></pre><p>And getting the following error:</p><pre><code>UnicodeEncodeError: ascii codec cant encode character u\\u03b1 in position 20: ordinal not in range(128)</code></pre><ul><li>Is there any way to get around this easily (i.e. I have unicode characters in my data frame)?</li><li>And is there a way to write to a tab delimited file instead of a CSV using e.g. a to-tab method (that I dont think exists)?</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'csv', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Writing a pandas DataFrame to CSV file\"\n",
    "    __QUESTION = \"<p>I have a dataframe in pandas which I would like to write to a CSV file.</p><p>I am doing this using:</p><pre><code>df.to_csv(out.csv)</code></pre><p>And getting the following error:</p><pre><code>UnicodeEncodeError: ascii codec cant encode character u\\u03b1 in position 20: ordinal not in range(128)</code></pre><ul><li>Is there any way to get around this easily (i.e. I have unicode characters in my data frame)?</li><li>And is there a way to write to a tab delimited file instead of a CSV using e.g. a to-tab method (that I dont think exists)?</li></ul>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16923281/writing-a-pandas-dataframe-to-csv-file\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc2580",
   "metadata": {},
   "source": [
    "### 19: How do I expand the output display to see more columns of a Pandas DataFrame?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3166e50",
   "metadata": {},
   "source": [
    "<p>Is there a way to widen the display of output in either interactive or script-execution mode?</p><p>Specifically, I am using the <code>describe()</code> function on a Pandas <code>DataFrame</code>.  When the <code>DataFrame</code> is five columns (labels) wide, I get the descriptive statistics that I want.  However, if the <code>DataFrame</code> has any more columns, the statistics are suppressed and something like this is returned:</p><pre class=lang-none prettyprint-override><code>&gt;&gt; Index: 8 entries, count to max&gt;&gt; Data columns:&gt;&gt; x1          8  non-null values&gt;&gt; x2          8  non-null values&gt;&gt; x3          8  non-null values&gt;&gt; x4          8  non-null values&gt;&gt; x5          8  non-null values&gt;&gt; x6          8  non-null values&gt;&gt; x7          8  non-null values</code></pre><p>The &quot;8&quot; value is given whether there are 6 or 7 columns.  What does the &quot;8&quot; refer to?</p><p>I have already tried dragging the <a href=https://en.wikipedia.org/wiki/IDLE rel=noreferrer>IDLE</a> window larger, as well as increasing the &quot;Configure IDLE&quot; width options, to no avail.</p><p>My purpose in using Pandas and <code>describe()</code> is to avoid using a second program like Stata to do basic data manipulation and investigation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'printing', 'column-width']\n",
    "    __TITLE = \"How do I expand the output display to see more columns of a Pandas DataFrame?\"\n",
    "    __QUESTION = \"<p>Is there a way to widen the display of output in either interactive or script-execution mode?</p><p>Specifically, I am using the <code>describe()</code> function on a Pandas <code>DataFrame</code>.  When the <code>DataFrame</code> is five columns (labels) wide, I get the descriptive statistics that I want.  However, if the <code>DataFrame</code> has any more columns, the statistics are suppressed and something like this is returned:</p><pre class=lang-none prettyprint-override><code>&gt;&gt; Index: 8 entries, count to max&gt;&gt; Data columns:&gt;&gt; x1          8  non-null values&gt;&gt; x2          8  non-null values&gt;&gt; x3          8  non-null values&gt;&gt; x4          8  non-null values&gt;&gt; x5          8  non-null values&gt;&gt; x6          8  non-null values&gt;&gt; x7          8  non-null values</code></pre><p>The &quot;8&quot; value is given whether there are 6 or 7 columns.  What does the &quot;8&quot; refer to?</p><p>I have already tried dragging the <a href=https://en.wikipedia.org/wiki/IDLE rel=noreferrer>IDLE</a> window larger, as well as increasing the &quot;Configure IDLE&quot; width options, to no avail.</p><p>My purpose in using Pandas and <code>describe()</code> is to avoid using a second program like Stata to do basic data manipulation and investigation.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa64fc",
   "metadata": {},
   "source": [
    "### 20: Deleting DataFrame row in Pandas based on column value\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9642227a",
   "metadata": {},
   "source": [
    "<p>I have the following DataFrame:</p><pre><code>             daysago  line_race rating        rw    wrating line_date                                                  2007-03-31       62         11     56  1.000000  56.000000 2007-03-10       83         11     67  1.000000  67.000000 2007-02-10      111          9     66  1.000000  66.000000 2007-01-13      139         10     83  0.880678  73.096278 2006-12-23      160         10     88  0.793033  69.786942 2006-11-09      204          9     52  0.636655  33.106077 2006-10-22      222          8     66  0.581946  38.408408 2006-09-29      245          9     70  0.518825  36.317752 2006-09-16      258         11     68  0.486226  33.063381 2006-08-30      275          8     72  0.446667  32.160051 2006-02-11      475          5     65  0.164591  10.698423 2006-01-13      504          0     70  0.142409   9.968634 2006-01-02      515          0     64  0.134800   8.627219 2005-12-06      542          0     70  0.117803   8.246238 2005-11-29      549          0     70  0.113758   7.963072 2005-11-22      556          0     -1  0.109852  -0.109852 2005-11-01      577          0     -1  0.098919  -0.098919 2005-10-20      589          0     -1  0.093168  -0.093168 2005-09-27      612          0     -1  0.083063  -0.083063 2005-09-07      632          0     -1  0.075171  -0.075171 2005-06-12      719          0     69  0.048690   3.359623 2005-05-29      733          0     -1  0.045404  -0.045404 2005-05-02      760          0     -1  0.039679  -0.039679 2005-04-02      790          0     -1  0.034160  -0.034160 2005-03-13      810          0     -1  0.030915  -0.030915 2004-11-09      934          0     -1  0.016647  -0.016647</code></pre><p>I need to remove the rows where <code>line_race</code> is equal to <code>0</code>. Whats the most efficient way to do this?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f487709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"Deleting DataFrame row in Pandas based on column value\"\n",
    "    __QUESTION = \"<p>I have the following DataFrame:</p><pre><code>             daysago  line_race rating        rw    wrating line_date                                                  2007-03-31       62         11     56  1.000000  56.000000 2007-03-10       83         11     67  1.000000  67.000000 2007-02-10      111          9     66  1.000000  66.000000 2007-01-13      139         10     83  0.880678  73.096278 2006-12-23      160         10     88  0.793033  69.786942 2006-11-09      204          9     52  0.636655  33.106077 2006-10-22      222          8     66  0.581946  38.408408 2006-09-29      245          9     70  0.518825  36.317752 2006-09-16      258         11     68  0.486226  33.063381 2006-08-30      275          8     72  0.446667  32.160051 2006-02-11      475          5     65  0.164591  10.698423 2006-01-13      504          0     70  0.142409   9.968634 2006-01-02      515          0     64  0.134800   8.627219 2005-12-06      542          0     70  0.117803   8.246238 2005-11-29      549          0     70  0.113758   7.963072 2005-11-22      556          0     -1  0.109852  -0.109852 2005-11-01      577          0     -1  0.098919  -0.098919 2005-10-20      589          0     -1  0.093168  -0.093168 2005-09-27      612          0     -1  0.083063  -0.083063 2005-09-07      632          0     -1  0.075171  -0.075171 2005-06-12      719          0     69  0.048690   3.359623 2005-05-29      733          0     -1  0.045404  -0.045404 2005-05-02      760          0     -1  0.039679  -0.039679 2005-04-02      790          0     -1  0.034160  -0.034160 2005-03-13      810          0     -1  0.030915  -0.030915 2004-11-09      934          0     -1  0.016647  -0.016647</code></pre><p>I need to remove the rows where <code>line_race</code> is equal to <code>0</code>. Whats the most efficient way to do this?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3f4a4",
   "metadata": {},
   "source": [
    "### 21: Combine two columns of text in pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2cb4b",
   "metadata": {},
   "source": [
    "<p>I have a 20 x 4000 dataframe in Python using pandas. Two of these columns are named <code>Year</code> and <code>quarter</code>. Id like to create a variable called <code>period</code> that makes <code>Year = 2000</code> and <code>quarter= q2</code> into <code>2000q2</code>.</p><p>Can anyone help with that?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Combine two columns of text in pandas dataframe\"\n",
    "    __QUESTION = \"<p>I have a 20 x 4000 dataframe in Python using pandas. Two of these columns are named <code>Year</code> and <code>quarter</code>. Id like to create a variable called <code>period</code> that makes <code>Year = 2000</code> and <code>quarter= q2</code> into <code>2000q2</code>.</p><p>Can anyone help with that?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88de211",
   "metadata": {},
   "source": [
    "### 22: How are iloc and loc different?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65dead",
   "metadata": {},
   "source": [
    "<p>Can someone explain how these two methods of slicing are different?<br />Ive seen <a href=http://pandas.pydata.org/pandas-docs/stable/indexing.html rel=noreferrer>the docs</a>,and Ive seen <a href=https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat>these</a> <a href=https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i>answers</a>, but I still find myself unable to understand how the three are different. To me, they seem interchangeable in large part, because they are at the lower levels of slicing.</p><p>For example, say we want to get the first five rows of a <code>DataFrame</code>.  How is it that these two work?</p><pre><code>df.loc[:5]df.iloc[:5]</code></pre><p>Can someone present three cases where the distinction in uses are clearer?</p><hr /><p>Once upon a time, I also wanted to know how these two functions differ from <code>df.ix[:5]</code> but <code>ix</code> has been removed from pandas 1.0, so I dont care anymore.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'indexing', 'pandas-loc']\n",
    "    __TITLE = \"How are iloc and loc different?\"\n",
    "    __QUESTION = \"<p>Can someone explain how these two methods of slicing are different?<br />Ive seen <a href=http://pandas.pydata.org/pandas-docs/stable/indexing.html rel=noreferrer>the docs</a>,and Ive seen <a href=https://stackoverflow.com/questions/28757389/loc-vs-iloc-vs-ix-vs-at-vs-iat>these</a> <a href=https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i>answers</a>, but I still find myself unable to understand how the three are different. To me, they seem interchangeable in large part, because they are at the lower levels of slicing.</p><p>For example, say we want to get the first five rows of a <code>DataFrame</code>.  How is it that these two work?</p><pre><code>df.loc[:5]df.iloc[:5]</code></pre><p>Can someone present three cases where the distinction in uses are clearer?</p><hr /><p>Once upon a time, I also wanted to know how these two functions differ from <code>df.ix[:5]</code> but <code>ix</code> has been removed from pandas 1.0, so I dont care anymore.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207f258",
   "metadata": {},
   "source": [
    "### 23: Pandas Merging 101\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e8b6b",
   "metadata": {},
   "source": [
    "<ul><li>How can I perform a (<code>INNER</code>| (<code>LEFT</code>|<code>RIGHT</code>|<code>FULL</code>) <code>OUTER</code>) <code>JOIN</code> with pandas?</li><li>How do I add NaNs for missing rows after a merge?</li><li>How do I get rid of NaNs after merging?</li><li>Can I merge on the index?</li><li>How do I merge multiple DataFrames?</li><li>Cross join with pandas</li><li><code>merge</code>? <code>join</code>? <code>concat</code>? <code>update</code>? Who? What? Why?!</li></ul><p>... and more. Ive seen these recurring questions asking about various facets of the pandas merge functionality. Most of the information regarding merge and its various use cases today is fragmented across dozens of badly worded, unsearchable posts. The aim here is to collate some of the more important points for posterity.</p><p>This Q&amp;A is meant to be the next installment in a series of helpful user guides on common pandas idioms (see <a href=https://stackoverflow.com/questions/47152691/how-to-pivot-a-dataframe>this post on pivoting</a>, and <a href=https://stackoverflow.com/questions/49620538/what-are-the-levels-keys-and-names-arguments-for-in-pandas-concat-functio>this post on concatenation</a>, which I will be touching on, later).</p><p>Please note that this post is <em>not</em> meant to be a replacement for <a href=https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html rel=noreferrer>the documentation</a>, so please read that as well! Some of the examples are taken from there.</p><hr /><h3>Table of Contents</h3><p><sub>For ease of access.</sub></p><ul><li><p><a href=https://stackoverflow.com/a/53645883/4909087>Merging basics - basic types of joins</a> (read this first)</p></li><li><p><a href=https://stackoverflow.com/a/65167356/4909087>Index-based joins</a></p></li><li><p><a href=https://stackoverflow.com/a/65167327/4909087>Generalizing to multiple DataFrames</a></p></li><li><p><a href=https://stackoverflow.com/a/53699013/4909087>Cross join</a></p></li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'join', 'merge', 'concatenation']\n",
    "    __TITLE = \"Pandas Merging 101\"\n",
    "    __QUESTION = \"<ul><li>How can I perform a (<code>INNER</code>| (<code>LEFT</code>|<code>RIGHT</code>|<code>FULL</code>) <code>OUTER</code>) <code>JOIN</code> with pandas?</li><li>How do I add NaNs for missing rows after a merge?</li><li>How do I get rid of NaNs after merging?</li><li>Can I merge on the index?</li><li>How do I merge multiple DataFrames?</li><li>Cross join with pandas</li><li><code>merge</code>? <code>join</code>? <code>concat</code>? <code>update</code>? Who? What? Why?!</li></ul><p>... and more. Ive seen these recurring questions asking about various facets of the pandas merge functionality. Most of the information regarding merge and its various use cases today is fragmented across dozens of badly worded, unsearchable posts. The aim here is to collate some of the more important points for posterity.</p><p>This Q&amp;A is meant to be the next installment in a series of helpful user guides on common pandas idioms (see <a href=https://stackoverflow.com/questions/47152691/how-to-pivot-a-dataframe>this post on pivoting</a>, and <a href=https://stackoverflow.com/questions/49620538/what-are-the-levels-keys-and-names-arguments-for-in-pandas-concat-functio>this post on concatenation</a>, which I will be touching on, later).</p><p>Please note that this post is <em>not</em> meant to be a replacement for <a href=https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html rel=noreferrer>the documentation</a>, so please read that as well! Some of the examples are taken from there.</p><hr /><h3>Table of Contents</h3><p><sub>For ease of access.</sub></p><ul><li><p><a href=https://stackoverflow.com/a/53645883/4909087>Merging basics - basic types of joins</a> (read this first)</p></li><li><p><a href=https://stackoverflow.com/a/65167356/4909087>Index-based joins</a></p></li><li><p><a href=https://stackoverflow.com/a/65167327/4909087>Generalizing to multiple DataFrames</a></p></li><li><p><a href=https://stackoverflow.com/a/53699013/4909087>Cross join</a></p></li></ul>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/53645882/pandas-merging-101\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8f45d",
   "metadata": {},
   "source": [
    "### 24: Creating an empty Pandas DataFrame, then filling it?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ab80a",
   "metadata": {},
   "source": [
    "<p>Im starting from the pandas DataFrame docs here: <a href=http://pandas.pydata.org/pandas-docs/stable/dsintro.html rel=noreferrer>http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p><p>Id like to iteratively fill the DataFrame with values in a time series kind of calculation.So basically, Id like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.</p><p>Id then add initial values and go over this data calculating the new row from the row before, say <code>row[A][t] = row[A][t-1]+1</code> or so.</p><p>Im currently using the code as below, but I feel its kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.Note: Im using Python 2.7.</p><pre><code>import datetime as dtimport pandas as pdimport scipy as sif __name__ == __main__:    base = dt.datetime.today().date()    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]    dates.sort()    valdict = {}    symbols = [A,B, C]    for symb in symbols:        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )    for thedate in dates:        if thedate &gt; dates[0]:            for symb in valdict:                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]    print valdict</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d71f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'dataframe', 'pandas']\n",
    "    __TITLE = \"Creating an empty Pandas DataFrame, then filling it?\"\n",
    "    __QUESTION = \"<p>Im starting from the pandas DataFrame docs here: <a href=http://pandas.pydata.org/pandas-docs/stable/dsintro.html rel=noreferrer>http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p><p>Id like to iteratively fill the DataFrame with values in a time series kind of calculation.So basically, Id like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.</p><p>Id then add initial values and go over this data calculating the new row from the row before, say <code>row[A][t] = row[A][t-1]+1</code> or so.</p><p>Im currently using the code as below, but I feel its kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.Note: Im using Python 2.7.</p><pre><code>import datetime as dtimport pandas as pdimport scipy as sif __name__ == __main__:    base = dt.datetime.today().date()    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]    dates.sort()    valdict = {}    symbols = [A,B, C]    for symb in symbols:        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )    for thedate in dates:        if thedate &gt; dates[0]:            for symb in valdict:                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]    print valdict</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9907f8a",
   "metadata": {},
   "source": [
    "### 25: Filter pandas DataFrame by substring criteria\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33878b92",
   "metadata": {},
   "source": [
    "<p>I have a pandas DataFrame with a column of string values. I need to select rows based on partial string matches.</p><p>Something like this idiom:</p><pre><code>re.search(pattern, cell_in_question) </code></pre><p>returning a boolean. I am familiar with the syntax of <code>df[df[A] == &quot;hello world&quot;]</code> but cant seem to find a way to do the same with a partial string match, say <code>hello</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'string', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Filter pandas DataFrame by substring criteria\"\n",
    "    __QUESTION = \"<p>I have a pandas DataFrame with a column of string values. I need to select rows based on partial string matches.</p><p>Something like this idiom:</p><pre><code>re.search(pattern, cell_in_question) </code></pre><p>returning a boolean. I am familiar with the syntax of <code>df[df[A] == &quot;hello world&quot;]</code> but cant seem to find a way to do the same with a partial string match, say <code>hello</code>.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11350770/filter-pandas-dataframe-by-substring-criteria\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613050e1",
   "metadata": {},
   "source": [
    "### 26: Shuffle DataFrame rows\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1ade4",
   "metadata": {},
   "source": [
    "<p>I have the following DataFrame:</p><pre><code>    Col1  Col2  Col3  Type0      1     2     3     11      4     5     6     1...20     7     8     9     221    10    11    12     2...45    13    14    15     346    16    17    18     3...</code></pre><p>The DataFrame is read from a CSV file. All rows which have <code>Type</code> 1 are on top, followed by the rows with <code>Type</code> 2, followed by the rows with <code>Type</code> 3, etc.</p><p>I would like to shuffle the order of the DataFrames rows so that all <code>Type</code>s are mixed. A possible result could be:</p><pre><code>    Col1  Col2  Col3  Type0      7     8     9     21     13    14    15     3...20     1     2     3     121    10    11    12     2...45     4     5     6     146    16    17    18     3...</code></pre><p>How can I achieve this?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'permutation', 'shuffle']\n",
    "    __TITLE = \"Shuffle DataFrame rows\"\n",
    "    __QUESTION = \"<p>I have the following DataFrame:</p><pre><code>    Col1  Col2  Col3  Type0      1     2     3     11      4     5     6     1...20     7     8     9     221    10    11    12     2...45    13    14    15     346    16    17    18     3...</code></pre><p>The DataFrame is read from a CSV file. All rows which have <code>Type</code> 1 are on top, followed by the rows with <code>Type</code> 2, followed by the rows with <code>Type</code> 3, etc.</p><p>I would like to shuffle the order of the DataFrames rows so that all <code>Type</code>s are mixed. A possible result could be:</p><pre><code>    Col1  Col2  Col3  Type0      7     8     9     21     13    14    15     3...20     1     2     3     121    10    11    12     2...45     4     5     6     146    16    17    18     3...</code></pre><p>How can I achieve this?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830becee",
   "metadata": {},
   "source": [
    "### 27: How to filter Pandas dataframe using &#39;in&#39; and &#39;not in&#39; like in SQL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e321f",
   "metadata": {},
   "source": [
    "<p>How can I achieve the equivalents of SQLs <code>IN</code> and <code>NOT IN</code>?</p><p>I have a list with the required values.Heres the scenario:</p><pre><code>df = pd.DataFrame({country: [US, UK, Germany, China]})countries_to_keep = [UK, China]# pseudo-code:df[df[country] not in countries_to_keep]</code></pre><p>My current way of doing this is as follows:</p><pre><code>df = pd.DataFrame({country: [US, UK, Germany, China]})df2 = pd.DataFrame({country: [UK, China], matched: True})# INdf.merge(df2, how=inner, on=country)# NOT INnot_in = df.merge(df2, how=left, on=country)not_in = not_in[pd.isnull(not_in[matched])]</code></pre><p>But this seems like a horrible kludge. Can anyone improve on it?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'sql-function']\n",
    "    __TITLE = \"How to filter Pandas dataframe using &#39;in&#39; and &#39;not in&#39; like in SQL\"\n",
    "    __QUESTION = \"<p>How can I achieve the equivalents of SQLs <code>IN</code> and <code>NOT IN</code>?</p><p>I have a list with the required values.Heres the scenario:</p><pre><code>df = pd.DataFrame({country: [US, UK, Germany, China]})countries_to_keep = [UK, China]# pseudo-code:df[df[country] not in countries_to_keep]</code></pre><p>My current way of doing this is as follows:</p><pre><code>df = pd.DataFrame({country: [US, UK, Germany, China]})df2 = pd.DataFrame({country: [UK, China], matched: True})# INdf.merge(df2, how=inner, on=country)# NOT INnot_in = df.merge(df2, how=left, on=country)not_in = not_in[pd.isnull(not_in[matched])]</code></pre><p>But this seems like a horrible kludge. Can anyone improve on it?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19960077/how-to-filter-pandas-dataframe-using-in-and-not-in-like-in-sql\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356c006",
   "metadata": {},
   "source": [
    "### 28: How to convert index of a pandas dataframe into a column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3762916",
   "metadata": {},
   "source": [
    "<p>This seems rather obvious, but I cant seem to figure out how to convert an index of data frame to a column?</p><p>For example:</p><pre><code>df=        gi       ptt_loc 0  384444683      593   1  384444684      594  2  384444686      596  </code></pre><p>To,</p><pre><code>df=    index1    gi       ptt_loc 0  0     384444683      593   1  1     384444684      594  2  2     384444686      596  </code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'indexing', 'series']\n",
    "    __TITLE = \"How to convert index of a pandas dataframe into a column\"\n",
    "    __QUESTION = \"<p>This seems rather obvious, but I cant seem to figure out how to convert an index of data frame to a column?</p><p>For example:</p><pre><code>df=        gi       ptt_loc 0  384444683      593   1  384444684      594  2  384444686      596  </code></pre><p>To,</p><pre><code>df=    index1    gi       ptt_loc 0  0     384444683      593   1  1     384444684      594  2  2     384444686      596  </code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20461165/how-to-convert-index-of-a-pandas-dataframe-into-a-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca986ba1",
   "metadata": {},
   "source": [
    "### 29: Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a9ce7",
   "metadata": {},
   "source": [
    "<p>I want to filter my dataframe with an <code>or</code> condition to keep rows with a particular columns values that are outside the range <code>[-0.25, 0.25]</code>. I tried:</p><pre><code>df = df[(df[col] &lt; -0.25) or (df[col] &gt; 0.25)]</code></pre><p>But I get the error:</p><blockquote><p>Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()</p></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1def62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'boolean', 'filtering']\n",
    "    __TITLE = \"Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\"\n",
    "    __QUESTION = \"<p>I want to filter my dataframe with an <code>or</code> condition to keep rows with a particular columns values that are outside the range <code>[-0.25, 0.25]</code>. I tried:</p><pre><code>df = df[(df[col] &lt; -0.25) or (df[col] &gt; 0.25)]</code></pre><p>But I get the error:</p><blockquote><p>Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()</p></blockquote>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c169bb",
   "metadata": {},
   "source": [
    "### 30: How to count the NaN values in a column in pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0c22e",
   "metadata": {},
   "source": [
    "<p>I want to find the number of <code>NaN</code> in each column of my data so that I can drop a column if it has fewer <code>NaN</code> than some threshold. I looked but wasnt able to find any function for this.  <a href=https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html rel=noreferrer><code>value_counts</code></a> is too slow for me because most of the values are distinct and Im only interested in the <code>NaN</code> count.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da326a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to count the NaN values in a column in pandas DataFrame\"\n",
    "    __QUESTION = \"<p>I want to find the number of <code>NaN</code> in each column of my data so that I can drop a column if it has fewer <code>NaN</code> than some threshold. I looked but wasnt able to find any function for this.  <a href=https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html rel=noreferrer><code>value_counts</code></a> is too slow for me because most of the values are distinct and Im only interested in the <code>NaN</code> count.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecadf58",
   "metadata": {},
   "source": [
    "### 31: Set value for particular cell in pandas DataFrame using index\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecff90b",
   "metadata": {},
   "source": [
    "<p>I have created a Pandas DataFrame</p><pre><code>df = DataFrame(index=[A,B,C], columns=[x,y])</code></pre><p>and have got this</p><pre>    x    yA  NaN  NaNB  NaN  NaNC  NaN  NaN</pre><p>Now, I would like to assign a value to particular cell, for example to row <code>C</code> and column <code>x</code>.I would expect to get this result:</p><pre>    x    yA  NaN  NaNB  NaN  NaNC  10  NaN</pre><p>with this code:</p><pre><code>df.xs(C)[x] = 10</code></pre><p>However, the contents of <code>df</code> has not changed. The dataframe contains yet again only <code>NaN</code>s.</p><p>Any suggestions?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd64a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'cell', 'nan']\n",
    "    __TITLE = \"Set value for particular cell in pandas DataFrame using index\"\n",
    "    __QUESTION = \"<p>I have created a Pandas DataFrame</p><pre><code>df = DataFrame(index=[A,B,C], columns=[x,y])</code></pre><p>and have got this</p><pre>    x    yA  NaN  NaNB  NaN  NaNC  NaN  NaN</pre><p>Now, I would like to assign a value to particular cell, for example to row <code>C</code> and column <code>x</code>.I would expect to get this result:</p><pre>    x    yA  NaN  NaNB  NaN  NaNC  10  NaN</pre><p>with this code:</p><pre><code>df.xs(C)[x] = 10</code></pre><p>However, the contents of <code>df</code> has not changed. The dataframe contains yet again only <code>NaN</code>s.</p><p>Any suggestions?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40af328",
   "metadata": {},
   "source": [
    "### 32: Get statistics for each group (such as count, mean, etc) using pandas GroupBy?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf44851",
   "metadata": {},
   "source": [
    "<p>I have a data frame <code>df</code> and I use several columns from it to <code>groupby</code>:</p><pre><code>df[col1,col2,col3,col4].groupby([col1,col2]).mean()</code></pre><p>In the above way I almost get the table (data frame) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many number were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.</p><p>In short: How do I get <strong>group-wise</strong> statistics for a dataframe?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3cd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'group-by', 'pandas-groupby']\n",
    "    __TITLE = \"Get statistics for each group (such as count, mean, etc) using pandas GroupBy?\"\n",
    "    __QUESTION = \"<p>I have a data frame <code>df</code> and I use several columns from it to <code>groupby</code>:</p><pre><code>df[col1,col2,col3,col4].groupby([col1,col2]).mean()</code></pre><p>In the above way I almost get the table (data frame) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many number were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.</p><p>In short: How do I get <strong>group-wise</strong> statistics for a dataframe?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19384532/get-statistics-for-each-group-such-as-count-mean-etc-using-pandas-groupby\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b0490",
   "metadata": {},
   "source": [
    "### 33: Import multiple csv files into pandas and concatenate into one DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd263ab",
   "metadata": {},
   "source": [
    "<p>I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:</p><pre><code>import globimport pandas as pd# get data file namespath =rC:\\DRO\\DCL_rawdata_filesfilenames = glob.glob(path + /*.csv)dfs = []for filename in filenames:    dfs.append(pd.read_csv(filename))# Concatenate all data into one DataFramebig_frame = pd.concat(dfs, ignore_index=True)</code></pre><p>I guess I need some help within the for loop???</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d114841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'csv', 'dataframe', 'concatenation']\n",
    "    __TITLE = \"Import multiple csv files into pandas and concatenate into one DataFrame\"\n",
    "    __QUESTION = \"<p>I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:</p><pre><code>import globimport pandas as pd# get data file namespath =rC:\\DRO\\DCL_rawdata_filesfilenames = glob.glob(path + /*.csv)dfs = []for filename in filenames:    dfs.append(pd.read_csv(filename))# Concatenate all data into one DataFramebig_frame = pd.concat(dfs, ignore_index=True)</code></pre><p>I guess I need some help within the for loop???</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9b8a5",
   "metadata": {},
   "source": [
    "### 34: How to avoid Python/Pandas creating an index in a saved csv?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24552e62",
   "metadata": {},
   "source": [
    "<p>I am trying to save a csv to a folder after making some edits to the file. </p><p>Every time I use <code>pd.to_csv(C:/Path of file.csv)</code> the csv file has a separate column of indexes. I want to avoid printing the index to csv.</p><p>I tried: </p><pre><code>pd.read_csv(C:/Path to file to edit.csv, index_col = False)</code></pre><p>And to save the file...</p><pre><code>pd.to_csv(C:/Path to save edited file.csv, index_col = False)</code></pre><p>However, I still got the unwanted index column. How can I avoid this when I save my files?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a17dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'csv', 'indexing', 'pandas']\n",
    "    __TITLE = \"How to avoid Python/Pandas creating an index in a saved csv?\"\n",
    "    __QUESTION = \"<p>I am trying to save a csv to a folder after making some edits to the file. </p><p>Every time I use <code>pd.to_csv(C:/Path of file.csv)</code> the csv file has a separate column of indexes. I want to avoid printing the index to csv.</p><p>I tried: </p><pre><code>pd.read_csv(C:/Path to file to edit.csv, index_col = False)</code></pre><p>And to save the file...</p><pre><code>pd.to_csv(C:/Path to save edited file.csv, index_col = False)</code></pre><p>However, I still got the unwanted index column. How can I avoid this when I save my files?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20845213/how-to-avoid-python-pandas-creating-an-index-in-a-saved-csv\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430096f2",
   "metadata": {},
   "source": [
    "### 35: Convert pandas dataframe to NumPy array\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e574511",
   "metadata": {},
   "source": [
    "<p>How do I convert a pandas dataframe into a NumPy array?</p><p>DataFrame:</p><pre><code>import numpy as npimport pandas as pdindex = [1, 2, 3, 4, 5, 6, 7]a = [np.nan, np.nan, np.nan, 0.1, 0.1, 0.1, 0.1]b = [0.2, np.nan, 0.2, 0.2, 0.2, np.nan, np.nan]c = [np.nan, 0.5, 0.5, np.nan, 0.5, 0.5, np.nan]df = pd.DataFrame({A: a, B: b, C: c}, index=index)df = df.rename_axis(ID)</code></pre><p>gives</p><pre><code>label   A    B    CID                                 1   NaN  0.2  NaN2   NaN  NaN  0.53   NaN  0.2  0.54   0.1  0.2  NaN5   0.1  0.2  0.56   0.1  NaN  0.57   0.1  NaN  NaN</code></pre><p>I would like to convert this to a NumPy array, like so:</p><pre><code>array([[ nan,  0.2,  nan],       [ nan,  nan,  0.5],       [ nan,  0.2,  0.5],       [ 0.1,  0.2,  nan],       [ 0.1,  0.2,  0.5],       [ 0.1,  nan,  0.5],       [ 0.1,  nan,  nan]])</code></pre><hr /><p>Also, is it possible to preserve the dtypes, like this?</p><pre><code>array([[ 1, nan,  0.2,  nan],       [ 2, nan,  nan,  0.5],       [ 3, nan,  0.2,  0.5],       [ 4, 0.1,  0.2,  nan],       [ 5, 0.1,  0.2,  0.5],       [ 6, 0.1,  nan,  0.5],       [ 7, 0.1,  nan,  nan]],     dtype=[(ID, &lt;i4), (A, &lt;f8), (B, &lt;f8), (B, &lt;f8)])</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4eb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'arrays', 'pandas', 'numpy', 'dataframe']\n",
    "    __TITLE = \"Convert pandas dataframe to NumPy array\"\n",
    "    __QUESTION = \"<p>How do I convert a pandas dataframe into a NumPy array?</p><p>DataFrame:</p><pre><code>import numpy as npimport pandas as pdindex = [1, 2, 3, 4, 5, 6, 7]a = [np.nan, np.nan, np.nan, 0.1, 0.1, 0.1, 0.1]b = [0.2, np.nan, 0.2, 0.2, 0.2, np.nan, np.nan]c = [np.nan, 0.5, 0.5, np.nan, 0.5, 0.5, np.nan]df = pd.DataFrame({A: a, B: b, C: c}, index=index)df = df.rename_axis(ID)</code></pre><p>gives</p><pre><code>label   A    B    CID                                 1   NaN  0.2  NaN2   NaN  NaN  0.53   NaN  0.2  0.54   0.1  0.2  NaN5   0.1  0.2  0.56   0.1  NaN  0.57   0.1  NaN  NaN</code></pre><p>I would like to convert this to a NumPy array, like so:</p><pre><code>array([[ nan,  0.2,  nan],       [ nan,  nan,  0.5],       [ nan,  0.2,  0.5],       [ 0.1,  0.2,  nan],       [ 0.1,  0.2,  0.5],       [ 0.1,  nan,  0.5],       [ 0.1,  nan,  nan]])</code></pre><hr /><p>Also, is it possible to preserve the dtypes, like this?</p><pre><code>array([[ 1, nan,  0.2,  nan],       [ 2, nan,  nan,  0.5],       [ 3, nan,  0.2,  0.5],       [ 4, 0.1,  0.2,  nan],       [ 5, 0.1,  0.2,  0.5],       [ 6, 0.1,  nan,  0.5],       [ 7, 0.1,  nan,  nan]],     dtype=[(ID, &lt;i4), (A, &lt;f8), (B, &lt;f8), (B, &lt;f8)])</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd438e9",
   "metadata": {},
   "source": [
    "### 36: Converting a Pandas GroupBy output from Series to DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddcf9cc",
   "metadata": {},
   "source": [
    "<p>Im starting with input data like this</p><pre><code>df1 = pandas.DataFrame( {     Name : [Alice, Bob, Mallory, Mallory, Bob , Mallory] ,     City : [Seattle, Seattle, Portland, Seattle, Seattle, Portland] } )</code></pre><p>Which when printed appears as this:</p><pre><code>   City     Name0   Seattle    Alice1   Seattle      Bob2  Portland  Mallory3   Seattle  Mallory4   Seattle      Bob5  Portland  Mallory</code></pre><p>Grouping is simple enough:</p><pre><code>g1 = df1.groupby( [ Name, City] ).count()</code></pre><p>and printing yields a <code>GroupBy</code> object:</p><pre><code>                  City  NameName    CityAlice   Seattle      1     1Bob     Seattle      2     2Mallory Portland     2     2        Seattle      1     1</code></pre><p>But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:</p><pre><code>                  City  NameName    CityAlice   Seattle      1     1Bob     Seattle      2     2Mallory Portland     2     2Mallory Seattle      1     1</code></pre><p>I cant quite see how to accomplish this in the pandas documentation. Any hints would be welcome.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b29667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'pandas-groupby', 'multi-index']\n",
    "    __TITLE = \"Converting a Pandas GroupBy output from Series to DataFrame\"\n",
    "    __QUESTION = \"<p>Im starting with input data like this</p><pre><code>df1 = pandas.DataFrame( {     Name : [Alice, Bob, Mallory, Mallory, Bob , Mallory] ,     City : [Seattle, Seattle, Portland, Seattle, Seattle, Portland] } )</code></pre><p>Which when printed appears as this:</p><pre><code>   City     Name0   Seattle    Alice1   Seattle      Bob2  Portland  Mallory3   Seattle  Mallory4   Seattle      Bob5  Portland  Mallory</code></pre><p>Grouping is simple enough:</p><pre><code>g1 = df1.groupby( [ Name, City] ).count()</code></pre><p>and printing yields a <code>GroupBy</code> object:</p><pre><code>                  City  NameName    CityAlice   Seattle      1     1Bob     Seattle      2     2Mallory Portland     2     2        Seattle      1     1</code></pre><p>But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:</p><pre><code>                  City  NameName    CityAlice   Seattle      1     1Bob     Seattle      2     2Mallory Portland     2     2Mallory Seattle      1     1</code></pre><p>I cant quite see how to accomplish this in the pandas documentation. Any hints would be welcome.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-output-from-series-to-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcb141",
   "metadata": {},
   "source": [
    "### 37: Difference between map, applymap and apply methods in Pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddc482",
   "metadata": {},
   "source": [
    "<p>Can you tell me when to use these vectorization methods with basic examples? </p><p>I see that <code>map</code> is a <code>Series</code> method whereas the rest are <code>DataFrame</code> methods. I got confused about <code>apply</code> and <code>applymap</code> methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'vectorization']\n",
    "    __TITLE = \"Difference between map, applymap and apply methods in Pandas\"\n",
    "    __QUESTION = \"<p>Can you tell me when to use these vectorization methods with basic examples? </p><p>I see that <code>map</code> is a <code>Series</code> method whereas the rest are <code>DataFrame</code> methods. I got confused about <code>apply</code> and <code>applymap</code> methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0c91f",
   "metadata": {},
   "source": [
    "### 38: How to check if any value is NaN in a Pandas DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6b453",
   "metadata": {},
   "source": [
    "<p>In Python Pandas, whats the best way to check whether a DataFrame has one (or more) NaN values?</p><p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas>This post</a> right here doesnt exactly answer my question either.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497470b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'nan']\n",
    "    __TITLE = \"How to check if any value is NaN in a Pandas DataFrame\"\n",
    "    __QUESTION = \"<p>In Python Pandas, whats the best way to check whether a DataFrame has one (or more) NaN values?</p><p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas>This post</a> right here doesnt exactly answer my question either.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7291af",
   "metadata": {},
   "source": [
    "### 39: How to apply a function to two columns of Pandas dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd14be",
   "metadata": {},
   "source": [
    "<p>Suppose I have a <code>df</code> which has columns of <code>ID, col_1, col_2</code>. And I define a function :</p><p><code>f = lambda x, y : my_function_expression</code>.</p><p>Now I want to apply the <code>f</code> to <code>df</code>s two columns <code>col_1, col_2</code> to element-wise calculate a new column <code>col_3</code> , somewhat like :</p><pre><code>df[col_3] = df[[col_1,col_2]].apply(f)  # Pandas gives : TypeError: (&lt;lambda&gt;() takes exactly 2 arguments (1 given)</code></pre><p>How to do ?</p><p><em><strong></em>**<em></strong> Add detail sample as below <strong></em>***</strong></p><pre><code>import pandas as pddf = pd.DataFrame({ID:[1,2,3], col_1: [0,2,3], col_2:[1,4,5]})mylist = [a,b,c,d,e,f]def get_sublist(sta,end):    return mylist[sta:end+1]#df[col_3] = df[[col_1,col_2]].apply(get_sublist,axis=1)# expect above to output df as below   ID  col_1  col_2            col_30  1      0      1       [a, b]1  2      2      4  [c, d, e]2  3      3      5  [d, e, f]</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to apply a function to two columns of Pandas dataframe\"\n",
    "    __QUESTION = \"<p>Suppose I have a <code>df</code> which has columns of <code>ID, col_1, col_2</code>. And I define a function :</p><p><code>f = lambda x, y : my_function_expression</code>.</p><p>Now I want to apply the <code>f</code> to <code>df</code>s two columns <code>col_1, col_2</code> to element-wise calculate a new column <code>col_3</code> , somewhat like :</p><pre><code>df[col_3] = df[[col_1,col_2]].apply(f)  # Pandas gives : TypeError: (&lt;lambda&gt;() takes exactly 2 arguments (1 given)</code></pre><p>How to do ?</p><p><em><strong></em>**<em></strong> Add detail sample as below <strong></em>***</strong></p><pre><code>import pandas as pddf = pd.DataFrame({ID:[1,2,3], col_1: [0,2,3], col_2:[1,4,5]})mylist = [a,b,c,d,e,f]def get_sublist(sta,end):    return mylist[sta:end+1]#df[col_3] = df[[col_1,col_2]].apply(get_sublist,axis=1)# expect above to output df as below   ID  col_1  col_2            col_30  1      0      1       [a, b]1  2      2      4  [c, d, e]2  3      3      5  [d, e, f]</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bba48",
   "metadata": {},
   "source": [
    "### 40: Constructing pandas DataFrame from values in variables gives &quot;ValueError: If using all scalar values, you must pass an index&quot;\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee94db",
   "metadata": {},
   "source": [
    "<p>This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.</p><pre><code>a = 2b = 3</code></pre><p>I want to construct a DataFrame from this:</p><pre><code>df2 = pd.DataFrame({A:a,B:b})</code></pre><p>This generates an error:  </p><blockquote>  <p>ValueError: If using all scalar values, you must pass an index</p></blockquote><p>I tried this also:</p><pre><code>df2 = (pd.DataFrame({a:a,b:b})).reset_index()</code></pre><p>This gives the same error message.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'scalar']\n",
    "    __TITLE = \"Constructing pandas DataFrame from values in variables gives &quot;ValueError: If using all scalar values, you must pass an index&quot;\"\n",
    "    __QUESTION = \"<p>This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.</p><pre><code>a = 2b = 3</code></pre><p>I want to construct a DataFrame from this:</p><pre><code>df2 = pd.DataFrame({A:a,B:b})</code></pre><p>This generates an error:  </p><blockquote>  <p>ValueError: If using all scalar values, you must pass an index</p></blockquote><p>I tried this also:</p><pre><code>df2 = (pd.DataFrame({a:a,b:b})).reset_index()</code></pre><p>This gives the same error message.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360360f",
   "metadata": {},
   "source": [
    "### 41: How to get a value from a cell of a dataframe?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563ed36",
   "metadata": {},
   "source": [
    "<p>I have constructed a condition that extract exactly one row from my data frame:</p><pre><code>d2 = df[(df[l_ext]==l_ext) &amp; (df[item]==item) &amp; (df[wn]==wn) &amp; (df[wd]==1)]</code></pre><p>Now I would like to take a value from a particular column:</p><pre><code>val = d2[col_name]</code></pre><p>But as a result I get a data frame that contains one row and one column (<em>i.e.</em> one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to get a value from a cell of a dataframe?\"\n",
    "    __QUESTION = \"<p>I have constructed a condition that extract exactly one row from my data frame:</p><pre><code>d2 = df[(df[l_ext]==l_ext) &amp; (df[item]==item) &amp; (df[wn]==wn) &amp; (df[wd]==1)]</code></pre><p>Now I would like to take a value from a particular column:</p><pre><code>val = d2[col_name]</code></pre><p>But as a result I get a data frame that contains one row and one column (<em>i.e.</em> one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16729574/how-to-get-a-value-from-a-cell-of-a-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65977a90",
   "metadata": {},
   "source": [
    "### 42: UnicodeDecodeError when reading CSV file in Pandas with Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db7970",
   "metadata": {},
   "source": [
    "<p>Im running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...</p><pre><code>File &quot;C:\\Importer\\src\\dfman\\importer.py&quot;, line 26, in import_chr     data = pd.read_csv(filepath, names=fields)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 400, in parser_f     return _read(filepath_or_buffer, kwds)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 205, in _read     return parser.read()   File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 608, in read     ret = self._engine.read(nrows)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 1028, in read     data = self._reader.read(nrows)File &quot;parser.pyx&quot;, line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)File &quot;parser.pyx&quot;, line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)File &quot;parser.pyx&quot;, line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)File &quot;parser.pyx&quot;, line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)File &quot;parser.pyx&quot;, line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)File &quot;parser.pyx&quot;, line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)File &quot;parser.pyx&quot;, line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)File &quot;parser.pyx&quot;, line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)UnicodeDecodeError: utf-8 codec cant decode byte 0xda in position 6: invalid    continuation byte</code></pre><p>The source/creation of these files all come from the same place. Whats the best way to correct this to proceed with the import?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5005a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'csv', 'dataframe', 'unicode']\n",
    "    __TITLE = \"UnicodeDecodeError when reading CSV file in Pandas with Python\"\n",
    "    __QUESTION = \"<p>Im running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...</p><pre><code>File &quot;C:\\Importer\\src\\dfman\\importer.py&quot;, line 26, in import_chr     data = pd.read_csv(filepath, names=fields)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 400, in parser_f     return _read(filepath_or_buffer, kwds)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 205, in _read     return parser.read()   File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 608, in read     ret = self._engine.read(nrows)File &quot;C:\\Python33\\lib\\site-packages\\pandas\\io\\parsers.py&quot;, line 1028, in read     data = self._reader.read(nrows)File &quot;parser.pyx&quot;, line 706, in pandas.parser.TextReader.read (pandas\\parser.c:6745)File &quot;parser.pyx&quot;, line 728, in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:6964)File &quot;parser.pyx&quot;, line 804, in pandas.parser.TextReader._read_rows (pandas\\parser.c:7780)File &quot;parser.pyx&quot;, line 890, in pandas.parser.TextReader._convert_column_data (pandas\\parser.c:8793)File &quot;parser.pyx&quot;, line 950, in pandas.parser.TextReader._convert_tokens (pandas\\parser.c:9484)File &quot;parser.pyx&quot;, line 1026, in pandas.parser.TextReader._convert_with_dtype (pandas\\parser.c:10642)File &quot;parser.pyx&quot;, line 1046, in pandas.parser.TextReader._string_convert (pandas\\parser.c:10853)File &quot;parser.pyx&quot;, line 1278, in pandas.parser._string_box_utf8 (pandas\\parser.c:15657)UnicodeDecodeError: utf-8 codec cant decode byte 0xda in position 6: invalid    continuation byte</code></pre><p>The source/creation of these files all come from the same place. Whats the best way to correct this to proceed with the import?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90ac2a",
   "metadata": {},
   "source": [
    "### 43: How to replace NaN values by Zeroes in a column of a Pandas Dataframe?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046d820",
   "metadata": {},
   "source": [
    "<p>I have a Pandas Dataframe as below:</p><pre><code>      itm Date                  Amount 67    420 2012-09-30 00:00:00   6521168    421 2012-09-09 00:00:00   2942469    421 2012-09-16 00:00:00   2987770    421 2012-09-23 00:00:00   3099071    421 2012-09-30 00:00:00   6130372    485 2012-09-09 00:00:00   7178173    485 2012-09-16 00:00:00     NaN74    485 2012-09-23 00:00:00   1107275    485 2012-09-30 00:00:00  11370276    489 2012-09-09 00:00:00   6473177    489 2012-09-16 00:00:00     NaN</code></pre><p>When I try to apply a function to the Amount column, I get the following error:</p><pre><code>ValueError: cannot convert float NaN to integer</code></pre><p>I have tried applying a function using .isnan from the Math ModuleI have tried the pandas .replace attributeI tried the .sparse data attribute from pandas 0.9I have also tried if NaN == NaN statement in a function.I have also looked at this article <a href=https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r>How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles.All the methods I have tried have not worked or do not recognise NaN.Any Hints or solutions would be appreciated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'nan']\n",
    "    __TITLE = \"How to replace NaN values by Zeroes in a column of a Pandas Dataframe?\"\n",
    "    __QUESTION = \"<p>I have a Pandas Dataframe as below:</p><pre><code>      itm Date                  Amount 67    420 2012-09-30 00:00:00   6521168    421 2012-09-09 00:00:00   2942469    421 2012-09-16 00:00:00   2987770    421 2012-09-23 00:00:00   3099071    421 2012-09-30 00:00:00   6130372    485 2012-09-09 00:00:00   7178173    485 2012-09-16 00:00:00     NaN74    485 2012-09-23 00:00:00   1107275    485 2012-09-30 00:00:00  11370276    489 2012-09-09 00:00:00   6473177    489 2012-09-16 00:00:00     NaN</code></pre><p>When I try to apply a function to the Amount column, I get the following error:</p><pre><code>ValueError: cannot convert float NaN to integer</code></pre><p>I have tried applying a function using .isnan from the Math ModuleI have tried the pandas .replace attributeI tried the .sparse data attribute from pandas 0.9I have also tried if NaN == NaN statement in a function.I have also looked at this article <a href=https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r>How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles.All the methods I have tried have not worked or do not recognise NaN.Any Hints or solutions would be appreciated.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13295735/how-to-replace-nan-values-by-zeroes-in-a-column-of-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40e8ff",
   "metadata": {},
   "source": [
    "### 44: Python Pandas Error tokenizing data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd4eb3",
   "metadata": {},
   "source": [
    "<p>Im trying to use pandas to manipulate a .csv file but I get this error:</p><blockquote>  <p>pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12</p></blockquote><p>I have tried to read the pandas docs, but found nothing.</p><p>My code is simple:</p><pre><code>path = GOOG Key Ratios.csv#print(open(path).read())data = pd.read_csv(path)</code></pre><p>How can I resolve this? Should I use the <code>csv</code> module or another language ?</p><p>File is from <a href=http://financials.morningstar.com/ratios/r.html?t=GOOG&amp;region=usa&amp;culture=en-US rel=noreferrer>Morningstar</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'csv', 'pandas']\n",
    "    __TITLE = \"Python Pandas Error tokenizing data\"\n",
    "    __QUESTION = \"<p>Im trying to use pandas to manipulate a .csv file but I get this error:</p><blockquote>  <p>pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12</p></blockquote><p>I have tried to read the pandas docs, but found nothing.</p><p>My code is simple:</p><pre><code>path = GOOG Key Ratios.csv#print(open(path).read())data = pd.read_csv(path)</code></pre><p>How can I resolve this? Should I use the <code>csv</code> module or another language ?</p><p>File is from <a href=http://financials.morningstar.com/ratios/r.html?t=GOOG&amp;region=usa&amp;culture=en-US rel=noreferrer>Morningstar</a></p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba4177",
   "metadata": {},
   "source": [
    "### 45: How to delete rows from a pandas DataFrame based on a conditional expression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6b5de",
   "metadata": {},
   "source": [
    "<p>I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.</p><p>I expect to be able to do this (per <a href=https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row>this answer</a>):</p><pre><code>df[(len(df[column name]) &lt; 2)]</code></pre><p>but I just get the error:</p><pre><code>KeyError: uno item named False</code></pre><p>What am I doing wrong?</p><p>(Note: I know I can use <code>df.dropna()</code> to get rid of rows that contain any <code>NaN</code>, but I didnt see how to remove rows based on a conditional expression.)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c67763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"How to delete rows from a pandas DataFrame based on a conditional expression\"\n",
    "    __QUESTION = \"<p>I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.</p><p>I expect to be able to do this (per <a href=https://stackoverflow.com/questions/11881165/slice-pandas-dataframe-by-row>this answer</a>):</p><pre><code>df[(len(df[column name]) &lt; 2)]</code></pre><p>but I just get the error:</p><pre><code>KeyError: uno item named False</code></pre><p>What am I doing wrong?</p><p>(Note: I know I can use <code>df.dropna()</code> to get rid of rows that contain any <code>NaN</code>, but I didnt see how to remove rows based on a conditional expression.)</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330e31d",
   "metadata": {},
   "source": [
    "### 46: Filter dataframe rows if value in column is in a set list of values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149459c",
   "metadata": {},
   "source": [
    "<p>I have a Python pandas DataFrame <code>rpt</code>:</p><pre><code>rpt&lt;class pandas.core.frame.DataFrame&gt;MultiIndex: 47518 entries, (000002, 20120331) to (603366, 20091231)Data columns:STK_ID                    47518  non-null valuesSTK_Name                  47518  non-null valuesRPT_Date                  47518  non-null valuessales                     47518  non-null values</code></pre><p>I can filter the rows whose stock id is <code>600809</code> like this: <code>rpt[rpt[STK_ID] == 600809]</code></p><pre><code>&lt;class pandas.core.frame.DataFrame&gt;MultiIndex: 25 entries, (600809, 20120331) to (600809, 20060331)Data columns:STK_ID                    25  non-null valuesSTK_Name                  25  non-null valuesRPT_Date                  25  non-null valuessales                     25  non-null values</code></pre><p>and I want to get all the rows of some stocks together, such as <code>[600809,600141,600329]</code>. That means I want a syntax like this: </p><pre><code>stk_list = [600809,600141,600329]rst = rpt[rpt[STK_ID] in stk_list] # this does not works in pandas </code></pre><p>Since pandas not accept above command, how to achieve the target? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Filter dataframe rows if value in column is in a set list of values\"\n",
    "    __QUESTION = \"<p>I have a Python pandas DataFrame <code>rpt</code>:</p><pre><code>rpt&lt;class pandas.core.frame.DataFrame&gt;MultiIndex: 47518 entries, (000002, 20120331) to (603366, 20091231)Data columns:STK_ID                    47518  non-null valuesSTK_Name                  47518  non-null valuesRPT_Date                  47518  non-null valuessales                     47518  non-null values</code></pre><p>I can filter the rows whose stock id is <code>600809</code> like this: <code>rpt[rpt[STK_ID] == 600809]</code></p><pre><code>&lt;class pandas.core.frame.DataFrame&gt;MultiIndex: 25 entries, (600809, 20120331) to (600809, 20060331)Data columns:STK_ID                    25  non-null valuesSTK_Name                  25  non-null valuesRPT_Date                  25  non-null valuessales                     25  non-null values</code></pre><p>and I want to get all the rows of some stocks together, such as <code>[600809,600141,600329]</code>. That means I want a syntax like this: </p><pre><code>stk_list = [600809,600141,600329]rst = rpt[rpt[STK_ID] in stk_list] # this does not works in pandas </code></pre><p>Since pandas not accept above command, how to achieve the target? </p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/12065885/filter-dataframe-rows-if-value-in-column-is-in-a-set-list-of-values\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e2984",
   "metadata": {},
   "source": [
    "### 47: How to check whether a pandas DataFrame is empty?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05b67e",
   "metadata": {},
   "source": [
    "<p>How to check whether a pandas <code>DataFrame</code> is empty? In my case I want to print some message in terminal if the <code>DataFrame</code> is empty. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98860c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to check whether a pandas DataFrame is empty?\"\n",
    "    __QUESTION = \"<p>How to check whether a pandas <code>DataFrame</code> is empty? In my case I want to print some message in terminal if the <code>DataFrame</code> is empty. </p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19828822/how-to-check-whether-a-pandas-dataframe-is-empty\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2963db2",
   "metadata": {},
   "source": [
    "### 48: pandas create new column based on values from other columns / apply a function of multiple columns, row-wise\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936c255",
   "metadata": {},
   "source": [
    "<p>I want to apply my custom function (it uses an if-else ladder) to these six columns (<code>ERI_Hispanic</code>, <code>ERI_AmerInd_AKNatv</code>, <code>ERI_Asian</code>, <code>ERI_Black_Afr.Amer</code>, <code>ERI_HI_PacIsl</code>, <code>ERI_White</code>) in each row of my dataframe.</p><p>Ive tried different methods from other questions but still cant seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they cant be counted as anything else.  Even if they have a &quot;1&quot; in another ethnicity column they still are counted as Hispanic not two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and cant be counted as a unique ethnicity(except for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated.</p><p>Its almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.</p><p>From the dataframe below I need to calculate a new column based on the following spec in SQL:</p><p><strong>CRITERIA</strong></p><pre><code>IF [ERI_Hispanic] = 1 THEN RETURN “Hispanic”ELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) &gt; 1 THEN RETURN “Two or More”ELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN “A/I AK Native”ELSE IF [ERI_Asian] = 1 THEN RETURN “Asian”ELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN “Black/AA”ELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN “Haw/Pac Isl.”ELSE IF [ERI_White] = 1 THEN RETURN “White”</code></pre><p>Comment: If the ERI Flag for Hispanic is True (1), the employee is classified as “Hispanic”</p><p>Comment: If more than 1 non-Hispanic ERI Flag is true, return “Two or More”</p><p><strong>DATAFRAME</strong></p><pre><code>     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined0    MOST           JEFF        E       0               0           0               0               0               1           White1    CRUISE         TOM         E       0               0           0               1               0               0           White2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown3    DICAP          LEO                 0               0           0               0               0               1           Unknown4    BRANDO         MARLON      E       0               0           0               0               0               0           White5    HANKS          TOM         0                       0           0               0               0               1           Unknown6    DENIRO         ROBERT      E       0               1           0               0               0               1           White7    PACINO         AL          E       0               0           0               0               0               1           White8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'numpy', 'apply']\n",
    "    __TITLE = \"pandas create new column based on values from other columns / apply a function of multiple columns, row-wise\"\n",
    "    __QUESTION = \"<p>I want to apply my custom function (it uses an if-else ladder) to these six columns (<code>ERI_Hispanic</code>, <code>ERI_AmerInd_AKNatv</code>, <code>ERI_Asian</code>, <code>ERI_Black_Afr.Amer</code>, <code>ERI_HI_PacIsl</code>, <code>ERI_White</code>) in each row of my dataframe.</p><p>Ive tried different methods from other questions but still cant seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they cant be counted as anything else.  Even if they have a &quot;1&quot; in another ethnicity column they still are counted as Hispanic not two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and cant be counted as a unique ethnicity(except for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated.</p><p>Its almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.</p><p>From the dataframe below I need to calculate a new column based on the following spec in SQL:</p><p><strong>CRITERIA</strong></p><pre><code>IF [ERI_Hispanic] = 1 THEN RETURN “Hispanic”ELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) &gt; 1 THEN RETURN “Two or More”ELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN “A/I AK Native”ELSE IF [ERI_Asian] = 1 THEN RETURN “Asian”ELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN “Black/AA”ELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN “Haw/Pac Isl.”ELSE IF [ERI_White] = 1 THEN RETURN “White”</code></pre><p>Comment: If the ERI Flag for Hispanic is True (1), the employee is classified as “Hispanic”</p><p>Comment: If more than 1 non-Hispanic ERI Flag is true, return “Two or More”</p><p><strong>DATAFRAME</strong></p><pre><code>     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined0    MOST           JEFF        E       0               0           0               0               0               1           White1    CRUISE         TOM         E       0               0           0               1               0               0           White2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown3    DICAP          LEO                 0               0           0               0               0               1           Unknown4    BRANDO         MARLON      E       0               0           0               0               0               0           White5    HANKS          TOM         0                       0           0               0               0               1           Unknown6    DENIRO         ROBERT      E       0               1           0               0               0               1           White7    PACINO         AL          E       0               0           0               0               0               1           White8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4172932",
   "metadata": {},
   "source": [
    "### 49: Selecting a row of pandas series/dataframe by integer index\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de015de",
   "metadata": {},
   "source": [
    "<p>I am curious as to why <code>df[2]</code> is not supported, while <code>df.ix[2]</code> and <code>df[2:3]</code> both work. </p><pre><code>In [26]: df.ix[2]Out[26]: A    1.027680B    1.514210C   -1.466963D   -0.162339Name: 2000-01-03 00:00:00In [27]: df[2:3]Out[27]:                   A        B         C         D2000-01-03  1.02768  1.51421 -1.466963 -0.162339</code></pre><p>I would expect <code>df[2]</code> to work the same way as <code>df[2:3]</code> to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d40e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'indexing']\n",
    "    __TITLE = \"Selecting a row of pandas series/dataframe by integer index\"\n",
    "    __QUESTION = \"<p>I am curious as to why <code>df[2]</code> is not supported, while <code>df.ix[2]</code> and <code>df[2:3]</code> both work. </p><pre><code>In [26]: df.ix[2]Out[26]: A    1.027680B    1.514210C   -1.466963D   -0.162339Name: 2000-01-03 00:00:00In [27]: df[2:3]Out[27]:                   A        B         C         D2000-01-03  1.02768  1.51421 -1.466963 -0.162339</code></pre><p>I would expect <code>df[2]</code> to work the same way as <code>df[2:3]</code> to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16096627/selecting-a-row-of-pandas-series-dataframe-by-integer-index\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041a53f",
   "metadata": {},
   "source": [
    "### 50: How can I pivot a dataframe?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c050c",
   "metadata": {},
   "source": [
    "<ul><li>What is pivot?</li><li>How do I pivot?</li><li>Is this a pivot?</li><li>Long format to wide format?</li></ul><p>Ive seen a lot of questions that ask about pivot tables.  Even if they dont know that they are asking about pivot tables, they usually are.  It is virtually impossible to write a canonical question and answer that encompasses all aspects of pivoting...</p><p>... But Im going to give it a go.</p><hr /><p>The problem with existing questions and answers is that often the question is focused on a nuance that the OP has trouble generalizing in order to use a number of the existing good answers.  However, none of the answers attempt to give a comprehensive explanation (because its a daunting task)</p><p>Look a few examples from my <a href=https://www.google.com/search?q=how%20to%20pivot%20a%20pandas%20dataframe&amp;oq=How%20do%20I%20pivot%20a%20pandas%20dataframe rel=noreferrer><strong>Google Search</strong></a></p><ol><li><a href=https://stackoverflow.com/q/28337117/2336654>How to pivot a dataframe in Pandas?</a></li></ol><ul><li>Good question and answer.  But the answer only answers the specific question with little explanation.</li></ul><ol start=2><li><a href=https://stackoverflow.com/q/42708193/2336654>pandas pivot table to data frame</a></li></ol><ul><li>In this question, the OP is concerned with the output of the pivot.  Namely how the columns look.  OP wanted it to look like R.  This isnt very helpful for pandas users.</li></ul><ol start=3><li><a href=https://stackoverflow.com/q/11400181/2336654>pandas pivoting a dataframe, duplicate rows</a></li></ol><ul><li>Another decent question but the answer focuses on one method, namely <code>pd.DataFrame.pivot</code></li></ul><p>So whenever someone searches for <code>pivot</code> they get sporadic results that are likely not going to answer their specific question.</p><hr /><h1>Setup</h1><p>You may notice that I conspicuously named my columns and relevant column values to correspond with how Im going to pivot in the answers below.</p><pre><code>import numpy as npimport pandas as pdfrom numpy.core.defchararray import addnp.random.seed([3,1415])n = 20cols = np.array([key, row, item, col])arr1 = (np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str)df = pd.DataFrame(    add(cols, arr1), columns=cols).join(    pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix(val))print(df)     key   row   item   col  val0  val10   key0  row3  item1  col3  0.81  0.041   key1  row2  item1  col2  0.44  0.072   key1  row0  item1  col0  0.77  0.013   key0  row4  item0  col2  0.15  0.594   key1  row0  item2  col1  0.81  0.645   key1  row2  item2  col4  0.13  0.886   key2  row4  item1  col3  0.88  0.397   key1  row4  item1  col1  0.10  0.078   key1  row0  item2  col4  0.65  0.029   key1  row2  item0  col2  0.35  0.6110  key2  row0  item2  col1  0.40  0.8511  key2  row4  item1  col2  0.64  0.2512  key0  row2  item2  col3  0.50  0.4413  key0  row4  item1  col4  0.24  0.4614  key1  row3  item2  col3  0.28  0.1115  key0  row3  item1  col1  0.31  0.2316  key0  row0  item2  col3  0.86  0.0117  key0  row4  item0  col3  0.64  0.2118  key2  row2  item2  col0  0.13  0.4519  key0  row2  item0  col4  0.37  0.70</code></pre><h3>Question(s)</h3><ol><li><p>Why do I get <code>ValueError: Index contains duplicate entries, cannot reshape</code></p></li><li><p>How do I pivot <code>df</code> such that the <code>col</code> values are columns, <code>row</code> values are the index, and mean of <code>val0</code> are the values?</p><pre><code> col   col0   col1   col2   col3  col4 row row0  0.77  0.605    NaN  0.860  0.65 row2  0.13    NaN  0.395  0.500  0.25 row3   NaN  0.310    NaN  0.545   NaN row4   NaN  0.100  0.395  0.760  0.24</code></pre></li><li><p>How do I pivot <code>df</code> such that the <code>col</code> values are columns, <code>row</code> values are the index, mean of <code>val0</code> are the values, and missing values are <code>0</code>?</p><pre><code> col   col0   col1   col2   col3  col4 row row0  0.77  0.605  0.000  0.860  0.65 row2  0.13  0.000  0.395  0.500  0.25 row3  0.00  0.310  0.000  0.545  0.00 row4  0.00  0.100  0.395  0.760  0.24</code></pre></li><li><p>Can I get something other than <code>mean</code>, like maybe <code>sum</code>?</p><pre><code> col   col0  col1  col2  col3  col4 row row0  0.77  1.21  0.00  0.86  0.65 row2  0.13  0.00  0.79  0.50  0.50 row3  0.00  0.31  0.00  1.09  0.00 row4  0.00  0.10  0.79  1.52  0.24</code></pre></li><li><p>Can I do more that one aggregation at a time?</p><pre><code>        sum                          mean col   col0  col1  col2  col3  col4  col0   col1   col2   col3  col4 row row0  0.77  1.21  0.00  0.86  0.65  0.77  0.605  0.000  0.860  0.65 row2  0.13  0.00  0.79  0.50  0.50  0.13  0.000  0.395  0.500  0.25 row3  0.00  0.31  0.00  1.09  0.00  0.00  0.310  0.000  0.545  0.00 row4  0.00  0.10  0.79  1.52  0.24  0.00  0.100  0.395  0.760  0.24</code></pre></li><li><p>Can I aggregate over multiple value columns?</p><pre><code>       val0                             val1 col   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4 row row0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02 row2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79 row3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00 row4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46</code></pre></li><li><p>Can Subdivide by multiple columns?</p><pre><code> item item0             item1                         item2 col   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4 row row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65 row2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13 row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00 row4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00</code></pre></li><li><p>Or</p><pre><code> item      item0             item1                         item2 col        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4 key  row key0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00      row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00      row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00      row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00 key1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65      row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13      row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00      row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00 key2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00      row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00      row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00</code></pre></li><li><p>Can I aggregate the frequency in which the column and rows occur together, aka &quot;cross tabulation&quot;?</p><pre><code> col   col0  col1  col2  col3  col4 row row0     1     2     0     1     1 row2     1     0     2     1     2 row3     0     1     0     2     0 row4     0     1     2     2     1</code></pre></li><li><p>How do I convert a DataFrame from long to wide by pivoting on ONLY two columns? Given,</p><pre><code>np.random.seed([3, 1415])df2 = pd.DataFrame({A: list(aaaabbbc), B: np.random.choice(15, 8)})df2   A   B0  a   01  a  112  a   23  a  114  b  105  b  106  b  147  c   7</code></pre><p>The expected should look something like</p><pre><code>      a     b    c0   0.0  10.0  7.01  11.0  10.0  NaN2   2.0  14.0  NaN3  11.0   NaN  NaN</code></pre></li><li><p>How do I flatten the multiple index to single index after <code>pivot</code>?</p><p>From</p><pre><code>   1  2   1  1  2a  2  1  1b  2  1  0c  1  0  0</code></pre><p>To</p><pre><code>   1|1  2|1  2|2a    2    1    1b    2    1    0c    1    0    0</code></pre></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a04184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'group-by', 'pivot', 'pandas-groupby']\n",
    "    __TITLE = \"How can I pivot a dataframe?\"\n",
    "    __QUESTION = \"<ul><li>What is pivot?</li><li>How do I pivot?</li><li>Is this a pivot?</li><li>Long format to wide format?</li></ul><p>Ive seen a lot of questions that ask about pivot tables.  Even if they dont know that they are asking about pivot tables, they usually are.  It is virtually impossible to write a canonical question and answer that encompasses all aspects of pivoting...</p><p>... But Im going to give it a go.</p><hr /><p>The problem with existing questions and answers is that often the question is focused on a nuance that the OP has trouble generalizing in order to use a number of the existing good answers.  However, none of the answers attempt to give a comprehensive explanation (because its a daunting task)</p><p>Look a few examples from my <a href=https://www.google.com/search?q=how%20to%20pivot%20a%20pandas%20dataframe&amp;oq=How%20do%20I%20pivot%20a%20pandas%20dataframe rel=noreferrer><strong>Google Search</strong></a></p><ol><li><a href=https://stackoverflow.com/q/28337117/2336654>How to pivot a dataframe in Pandas?</a></li></ol><ul><li>Good question and answer.  But the answer only answers the specific question with little explanation.</li></ul><ol start=2><li><a href=https://stackoverflow.com/q/42708193/2336654>pandas pivot table to data frame</a></li></ol><ul><li>In this question, the OP is concerned with the output of the pivot.  Namely how the columns look.  OP wanted it to look like R.  This isnt very helpful for pandas users.</li></ul><ol start=3><li><a href=https://stackoverflow.com/q/11400181/2336654>pandas pivoting a dataframe, duplicate rows</a></li></ol><ul><li>Another decent question but the answer focuses on one method, namely <code>pd.DataFrame.pivot</code></li></ul><p>So whenever someone searches for <code>pivot</code> they get sporadic results that are likely not going to answer their specific question.</p><hr /><h1>Setup</h1><p>You may notice that I conspicuously named my columns and relevant column values to correspond with how Im going to pivot in the answers below.</p><pre><code>import numpy as npimport pandas as pdfrom numpy.core.defchararray import addnp.random.seed([3,1415])n = 20cols = np.array([key, row, item, col])arr1 = (np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str)df = pd.DataFrame(    add(cols, arr1), columns=cols).join(    pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix(val))print(df)     key   row   item   col  val0  val10   key0  row3  item1  col3  0.81  0.041   key1  row2  item1  col2  0.44  0.072   key1  row0  item1  col0  0.77  0.013   key0  row4  item0  col2  0.15  0.594   key1  row0  item2  col1  0.81  0.645   key1  row2  item2  col4  0.13  0.886   key2  row4  item1  col3  0.88  0.397   key1  row4  item1  col1  0.10  0.078   key1  row0  item2  col4  0.65  0.029   key1  row2  item0  col2  0.35  0.6110  key2  row0  item2  col1  0.40  0.8511  key2  row4  item1  col2  0.64  0.2512  key0  row2  item2  col3  0.50  0.4413  key0  row4  item1  col4  0.24  0.4614  key1  row3  item2  col3  0.28  0.1115  key0  row3  item1  col1  0.31  0.2316  key0  row0  item2  col3  0.86  0.0117  key0  row4  item0  col3  0.64  0.2118  key2  row2  item2  col0  0.13  0.4519  key0  row2  item0  col4  0.37  0.70</code></pre><h3>Question(s)</h3><ol><li><p>Why do I get <code>ValueError: Index contains duplicate entries, cannot reshape</code></p></li><li><p>How do I pivot <code>df</code> such that the <code>col</code> values are columns, <code>row</code> values are the index, and mean of <code>val0</code> are the values?</p><pre><code> col   col0   col1   col2   col3  col4 row row0  0.77  0.605    NaN  0.860  0.65 row2  0.13    NaN  0.395  0.500  0.25 row3   NaN  0.310    NaN  0.545   NaN row4   NaN  0.100  0.395  0.760  0.24</code></pre></li><li><p>How do I pivot <code>df</code> such that the <code>col</code> values are columns, <code>row</code> values are the index, mean of <code>val0</code> are the values, and missing values are <code>0</code>?</p><pre><code> col   col0   col1   col2   col3  col4 row row0  0.77  0.605  0.000  0.860  0.65 row2  0.13  0.000  0.395  0.500  0.25 row3  0.00  0.310  0.000  0.545  0.00 row4  0.00  0.100  0.395  0.760  0.24</code></pre></li><li><p>Can I get something other than <code>mean</code>, like maybe <code>sum</code>?</p><pre><code> col   col0  col1  col2  col3  col4 row row0  0.77  1.21  0.00  0.86  0.65 row2  0.13  0.00  0.79  0.50  0.50 row3  0.00  0.31  0.00  1.09  0.00 row4  0.00  0.10  0.79  1.52  0.24</code></pre></li><li><p>Can I do more that one aggregation at a time?</p><pre><code>        sum                          mean col   col0  col1  col2  col3  col4  col0   col1   col2   col3  col4 row row0  0.77  1.21  0.00  0.86  0.65  0.77  0.605  0.000  0.860  0.65 row2  0.13  0.00  0.79  0.50  0.50  0.13  0.000  0.395  0.500  0.25 row3  0.00  0.31  0.00  1.09  0.00  0.00  0.310  0.000  0.545  0.00 row4  0.00  0.10  0.79  1.52  0.24  0.00  0.100  0.395  0.760  0.24</code></pre></li><li><p>Can I aggregate over multiple value columns?</p><pre><code>       val0                             val1 col   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4 row row0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02 row2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79 row3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00 row4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46</code></pre></li><li><p>Can Subdivide by multiple columns?</p><pre><code> item item0             item1                         item2 col   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4 row row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65 row2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13 row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00 row4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00</code></pre></li><li><p>Or</p><pre><code> item      item0             item1                         item2 col        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4 key  row key0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00      row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00      row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00      row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00 key1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65      row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13      row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00      row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00 key2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00      row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00      row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00</code></pre></li><li><p>Can I aggregate the frequency in which the column and rows occur together, aka &quot;cross tabulation&quot;?</p><pre><code> col   col0  col1  col2  col3  col4 row row0     1     2     0     1     1 row2     1     0     2     1     2 row3     0     1     0     2     0 row4     0     1     2     2     1</code></pre></li><li><p>How do I convert a DataFrame from long to wide by pivoting on ONLY two columns? Given,</p><pre><code>np.random.seed([3, 1415])df2 = pd.DataFrame({A: list(aaaabbbc), B: np.random.choice(15, 8)})df2   A   B0  a   01  a  112  a   23  a  114  b  105  b  106  b  147  c   7</code></pre><p>The expected should look something like</p><pre><code>      a     b    c0   0.0  10.0  7.01  11.0  10.0  NaN2   2.0  14.0  NaN3  11.0   NaN  NaN</code></pre></li><li><p>How do I flatten the multiple index to single index after <code>pivot</code>?</p><p>From</p><pre><code>   1  2   1  1  2a  2  1  1b  2  1  0c  1  0  0</code></pre><p>To</p><pre><code>   1|1  2|1  2|2a    2    1    1b    2    1    0c    1    0    0</code></pre></li></ol>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/47152691/how-can-i-pivot-a-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e930356",
   "metadata": {},
   "source": [
    "### 51: Remap values in pandas column with a dict, preserve NaNs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6aabd",
   "metadata": {},
   "source": [
    "<p>I have a dictionary which looks like this: <code>di = {1: &quot;A&quot;, 2: &quot;B&quot;}</code></p><p>I would like to apply it to the <code>col1</code> column of a dataframe similar to:</p><pre><code>     col1   col20       w      a1       1      22       2    NaN</code></pre><p>to get:</p><pre><code>     col1   col20       w      a1       A      22       B    NaN</code></pre><p>How can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb881429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'dictionary', 'pandas', 'remap']\n",
    "    __TITLE = \"Remap values in pandas column with a dict, preserve NaNs\"\n",
    "    __QUESTION = \"<p>I have a dictionary which looks like this: <code>di = {1: &quot;A&quot;, 2: &quot;B&quot;}</code></p><p>I would like to apply it to the <code>col1</code> column of a dataframe similar to:</p><pre><code>     col1   col20       w      a1       1      22       2    NaN</code></pre><p>to get:</p><pre><code>     col1   col20       w      a1       A      22       B    NaN</code></pre><p>How can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict-preserve-nans\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdf76a",
   "metadata": {},
   "source": [
    "### 52: how to sort pandas dataframe from one column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020f2ed",
   "metadata": {},
   "source": [
    "<p>I have a data frame like this: </p><pre><code>print(df)        0          1     20   354.7      April   4.01    55.4     August   8.02   176.5   December  12.03    95.5   February   2.04    85.6    January   1.05     152       July   7.06   238.7       June   6.07   104.8      March   3.08   283.5        May   5.09   278.8   November  11.010  249.6    October  10.011  212.7  September   9.0</code></pre><p>As you can see, months are not in calendar order. So I created a second column to get the month number corresponding to each month (1-12). From there, how can I sort this data frame according to  calendar months order?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a538425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'sorting', 'time']\n",
    "    __TITLE = \"how to sort pandas dataframe from one column\"\n",
    "    __QUESTION = \"<p>I have a data frame like this: </p><pre><code>print(df)        0          1     20   354.7      April   4.01    55.4     August   8.02   176.5   December  12.03    95.5   February   2.04    85.6    January   1.05     152       July   7.06   238.7       June   6.07   104.8      March   3.08   283.5        May   5.09   278.8   November  11.010  249.6    October  10.011  212.7  September   9.0</code></pre><p>As you can see, months are not in calendar order. So I created a second column to get the month number corresponding to each month (1-12). From there, how can I sort this data frame according to  calendar months order?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-from-one-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688c53c",
   "metadata": {},
   "source": [
    "### 53: How to check if a column exists in Pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905d696",
   "metadata": {},
   "source": [
    "<p>How do I check if a column exists in a Pandas DataFrame <code>df</code>?</p><pre><code>   A   B    C0  3  40  1001  6  30  200</code></pre><p>How would I check that the column <code>&quot;A&quot;</code> exists in the above DataFrame so that I can compute:</p><pre><code>df[sum] = df[A] + df[C]</code></pre><p>And if <code>&quot;A&quot;</code> doesnt exist:</p><pre><code>df[sum] = df[B] + df[C]</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to check if a column exists in Pandas\"\n",
    "    __QUESTION = \"<p>How do I check if a column exists in a Pandas DataFrame <code>df</code>?</p><pre><code>   A   B    C0  3  40  1001  6  30  200</code></pre><p>How would I check that the column <code>&quot;A&quot;</code> exists in the above DataFrame so that I can compute:</p><pre><code>df[sum] = df[A] + df[C]</code></pre><p>And if <code>&quot;A&quot;</code> doesnt exist:</p><pre><code>df[sum] = df[B] + df[C]</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/24870306/how-to-check-if-a-column-exists-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e2f3d",
   "metadata": {},
   "source": [
    "### 54: How to flatten a hierarchical index in columns\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae1e06",
   "metadata": {},
   "source": [
    "<p>I have a data frame with a hierarchical index in axis 1 (columns) (from a <code>groupby.agg</code> operation):</p><pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf                                            sum   sum   sum    sum   amax   amin0  702730  26451  1993      1    1     1     0    12     13  30.92  24.981  702730  26451  1993      1    2     0     0    13     13  32.00  24.982  702730  26451  1993      1    3     1    10     2     13  23.00   6.983  702730  26451  1993      1    4     1     0    12     13  10.04   3.924  702730  26451  1993      1    5     3     0    10     13  19.94  10.94</code></pre><p>I want to flatten it, so that it looks like this (names arent critical - I could rename):</p><pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   0  702730  26451  1993      1    1     1     0    12     13  30.92          24.981  702730  26451  1993      1    2     0     0    13     13  32.00          24.982  702730  26451  1993      1    3     1    10     2     13  23.00          6.983  702730  26451  1993      1    4     1     0    12     13  10.04          3.924  702730  26451  1993      1    5     3     0    10     13  19.94          10.94</code></pre><p>How do I do this? (Ive tried a lot, to no avail.) </p><p>Per a suggestion, here is the head in dict form</p><pre><code>{(USAF, ): {0: 702730,  1: 702730,  2: 702730,  3: 702730,  4: 702730}, (WBAN, ): {0: 26451, 1: 26451, 2: 26451, 3: 26451, 4: 26451}, (day, ): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}, (month, ): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1}, (s_CD, sum): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0}, (s_CL, sum): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0}, (s_CNT, sum): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0}, (s_PC, sum): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0}, (tempf, amax): {0: 30.920000000000002,  1: 32.0,  2: 23.0,  3: 10.039999999999999,  4: 19.939999999999998}, (tempf, amin): {0: 24.98,  1: 24.98,  2: 6.9799999999999969,  3: 3.9199999999999982,  4: 10.940000000000001}, (year, ): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa557681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'multi-index']\n",
    "    __TITLE = \"How to flatten a hierarchical index in columns\"\n",
    "    __QUESTION = \"<p>I have a data frame with a hierarchical index in axis 1 (columns) (from a <code>groupby.agg</code> operation):</p><pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf                                            sum   sum   sum    sum   amax   amin0  702730  26451  1993      1    1     1     0    12     13  30.92  24.981  702730  26451  1993      1    2     0     0    13     13  32.00  24.982  702730  26451  1993      1    3     1    10     2     13  23.00   6.983  702730  26451  1993      1    4     1     0    12     13  10.04   3.924  702730  26451  1993      1    5     3     0    10     13  19.94  10.94</code></pre><p>I want to flatten it, so that it looks like this (names arent critical - I could rename):</p><pre><code>     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   0  702730  26451  1993      1    1     1     0    12     13  30.92          24.981  702730  26451  1993      1    2     0     0    13     13  32.00          24.982  702730  26451  1993      1    3     1    10     2     13  23.00          6.983  702730  26451  1993      1    4     1     0    12     13  10.04          3.924  702730  26451  1993      1    5     3     0    10     13  19.94          10.94</code></pre><p>How do I do this? (Ive tried a lot, to no avail.) </p><p>Per a suggestion, here is the head in dict form</p><pre><code>{(USAF, ): {0: 702730,  1: 702730,  2: 702730,  3: 702730,  4: 702730}, (WBAN, ): {0: 26451, 1: 26451, 2: 26451, 3: 26451, 4: 26451}, (day, ): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}, (month, ): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1}, (s_CD, sum): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0}, (s_CL, sum): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0}, (s_CNT, sum): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0}, (s_PC, sum): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0}, (tempf, amax): {0: 30.920000000000002,  1: 32.0,  2: 23.0,  3: 10.039999999999999,  4: 19.939999999999998}, (tempf, amin): {0: 24.98,  1: 24.98,  2: 6.9799999999999969,  3: 3.9199999999999982,  4: 10.940000000000001}, (year, ): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/14507794/how-to-flatten-a-hierarchical-index-in-columns\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898147b6",
   "metadata": {},
   "source": [
    "### 55: Convert Python dict into a dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483770e9",
   "metadata": {},
   "source": [
    "<p>I have a Python dictionary like the following:</p><pre><code>{u2012-06-08: 388, u2012-06-09: 388, u2012-06-10: 388, u2012-06-11: 389, u2012-06-12: 389, u2012-06-13: 389, u2012-06-14: 389, u2012-06-15: 389, u2012-06-16: 389, u2012-06-17: 389, u2012-06-18: 390, u2012-06-19: 390, u2012-06-20: 390, u2012-06-21: 390, u2012-06-22: 390, u2012-06-23: 390, u2012-06-24: 390, u2012-06-25: 391, u2012-06-26: 391, u2012-06-27: 391, u2012-06-28: 391, u2012-06-29: 391, u2012-06-30: 391, u2012-07-01: 391, u2012-07-02: 392, u2012-07-03: 392, u2012-07-04: 392, u2012-07-05: 392, u2012-07-06: 392}</code></pre><p>The keys are <a href=http://en.wikipedia.org/wiki/Unicode rel=noreferrer>Unicode</a> dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)</p><pre><code>     Date         DateValue0    2012-07-01    3911    2012-07-02    3922    2012-07-03    392.    2012-07-04    392.    ...           ....    ...           ...</code></pre><p>Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.</p><p>I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.</p><p>So far I have tried converting the dict into a series object but this doesnt seem to maintain the relationship between the columns:</p><pre><code>s  = Series(my_dict,index=my_dict.keys())</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Convert Python dict into a dataframe\"\n",
    "    __QUESTION = \"<p>I have a Python dictionary like the following:</p><pre><code>{u2012-06-08: 388, u2012-06-09: 388, u2012-06-10: 388, u2012-06-11: 389, u2012-06-12: 389, u2012-06-13: 389, u2012-06-14: 389, u2012-06-15: 389, u2012-06-16: 389, u2012-06-17: 389, u2012-06-18: 390, u2012-06-19: 390, u2012-06-20: 390, u2012-06-21: 390, u2012-06-22: 390, u2012-06-23: 390, u2012-06-24: 390, u2012-06-25: 391, u2012-06-26: 391, u2012-06-27: 391, u2012-06-28: 391, u2012-06-29: 391, u2012-06-30: 391, u2012-07-01: 391, u2012-07-02: 392, u2012-07-03: 392, u2012-07-04: 392, u2012-07-05: 392, u2012-07-06: 392}</code></pre><p>The keys are <a href=http://en.wikipedia.org/wiki/Unicode rel=noreferrer>Unicode</a> dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)</p><pre><code>     Date         DateValue0    2012-07-01    3911    2012-07-02    3922    2012-07-03    392.    2012-07-04    392.    ...           ....    ...           ...</code></pre><p>Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.</p><p>I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.</p><p>So far I have tried converting the dict into a series object but this doesnt seem to maintain the relationship between the columns:</p><pre><code>s  = Series(my_dict,index=my_dict.keys())</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43127ff",
   "metadata": {},
   "source": [
    "### 56: How to reset index in a pandas dataframe?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d6f6c",
   "metadata": {},
   "source": [
    "<p>I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: <code>[1,5,6,10,11]</code> and I would like to reset it to <code>[0,1,2,3,4]</code>. How can I do it?</p><hr><p>The following seems to work:</p><pre><code>df = df.reset_index()del df[index]</code></pre><p>The following does not work:</p><pre><code>df = df.reindex()</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'indexing', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to reset index in a pandas dataframe?\"\n",
    "    __QUESTION = \"<p>I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: <code>[1,5,6,10,11]</code> and I would like to reset it to <code>[0,1,2,3,4]</code>. How can I do it?</p><hr><p>The following seems to work:</p><pre><code>df = df.reset_index()del df[index]</code></pre><p>The following does not work:</p><pre><code>df = df.reindex()</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20490274/how-to-reset-index-in-a-pandas-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c6123",
   "metadata": {},
   "source": [
    "### 57: How to select all columns except one in pandas?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393df1f",
   "metadata": {},
   "source": [
    "<p>I have a dataframe that look like this:</p><pre><code>import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.rand(4,4), columns=list(abcd))df      a         b         c         d0  0.418762  0.042369  0.869203  0.9723141  0.991058  0.510228  0.594784  0.5343662  0.407472  0.259811  0.396664  0.8942023  0.726168  0.139531  0.324932  0.906575</code></pre><p>How I can get all columns except <code>b</code>?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"How to select all columns except one in pandas?\"\n",
    "    __QUESTION = \"<p>I have a dataframe that look like this:</p><pre><code>import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.rand(4,4), columns=list(abcd))df      a         b         c         d0  0.418762  0.042369  0.869203  0.9723141  0.991058  0.510228  0.594784  0.5343662  0.407472  0.259811  0.396664  0.8942023  0.726168  0.139531  0.324932  0.906575</code></pre><p>How I can get all columns except <code>b</code>?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a50cd",
   "metadata": {},
   "source": [
    "### 58: Get list from pandas dataframe column or row?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df0208",
   "metadata": {},
   "source": [
    "<p>I have a dataframe <code>df</code> imported from an Excel document like this:</p><pre><code>cluster load_date   budget  actual  fixed_priceA   1/1/2014    1000    4000    YA   2/1/2014    12000   10000   YA   3/1/2014    36000   2000    YB   4/1/2014    15000   10000   NB   4/1/2014    12000   11500   NB   4/1/2014    90000   11000   NC   7/1/2014    22000   18000   NC   8/1/2014    30000   28960   NC   9/1/2014    53000   51200   N</code></pre><p>I want to be able to return the contents of column 1 <code>df[cluster]</code> as a list, so I can run a for-loop over it, and create an Excel worksheet for every cluster.</p><p>Is it also possible to return the contents of a whole column or row to a list? e.g.</p><pre><code>list = [], list[column1] or list[df.ix(row1)]</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcf7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'list', 'pandas']\n",
    "    __TITLE = \"Get list from pandas dataframe column or row?\"\n",
    "    __QUESTION = \"<p>I have a dataframe <code>df</code> imported from an Excel document like this:</p><pre><code>cluster load_date   budget  actual  fixed_priceA   1/1/2014    1000    4000    YA   2/1/2014    12000   10000   YA   3/1/2014    36000   2000    YB   4/1/2014    15000   10000   NB   4/1/2014    12000   11500   NB   4/1/2014    90000   11000   NC   7/1/2014    22000   18000   NC   8/1/2014    30000   28960   NC   9/1/2014    53000   51200   N</code></pre><p>I want to be able to return the contents of column 1 <code>df[cluster]</code> as a list, so I can run a for-loop over it, and create an Excel worksheet for every cluster.</p><p>Is it also possible to return the contents of a whole column or row to a list? e.g.</p><pre><code>list = [], list[column1] or list[df.ix(row1)]</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column-or-row\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca7a1a",
   "metadata": {},
   "source": [
    "### 59: How can I use the apply() function for a single column?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39389bbc",
   "metadata": {},
   "source": [
    "<p>I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How can I use the apply() function for a single column?\"\n",
    "    __QUESTION = \"<p>I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/34962104/how-can-i-use-the-apply-function-for-a-single-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c44ada",
   "metadata": {},
   "source": [
    "### 60: How do I create test and train samples from one dataframe with pandas?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0e48b",
   "metadata": {},
   "source": [
    "<p>I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.</p><p>Thanks!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'python-2.7', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How do I create test and train samples from one dataframe with pandas?\"\n",
    "    __QUESTION = \"<p>I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.</p><p>Thanks!</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6dcc3",
   "metadata": {},
   "source": [
    "### 61: Pandas read_csv: low_memory and dtype options\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfade0f5",
   "metadata": {},
   "source": [
    "<pre><code>df = pd.read_csv(somefile.csv)</code></pre><p>...gives an error:</p><blockquote><p>.../site-packages/pandas/io/parsers.py:1130:DtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtypeoption on import or set low_memory=False.</p></blockquote><p>Why is the <code>dtype</code> option related to <code>low_memory</code>, and why might <code>low_memory=False</code> help?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'parsing', 'numpy', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Pandas read_csv: low_memory and dtype options\"\n",
    "    __QUESTION = \"<pre><code>df = pd.read_csv(somefile.csv)</code></pre><p>...gives an error:</p><blockquote><p>.../site-packages/pandas/io/parsers.py:1130:DtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtypeoption on import or set low_memory=False.</p></blockquote><p>Why is the <code>dtype</code> option related to <code>low_memory</code>, and why might <code>low_memory=False</code> help?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b6c15",
   "metadata": {},
   "source": [
    "### 62: How to group dataframe rows into list in pandas groupby\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3ae49",
   "metadata": {},
   "source": [
    "<p>I have a pandas data frame <code>df</code> like:</p><pre><code>a bA 1A 2B 5B 5B 4C 6</code></pre><p>I want to <strong>group by the first column and get second column as lists in rows</strong>:</p><pre><code>A [1,2]B [5,5,4]C [6]</code></pre><p>Is it possible to do something like this using pandas groupby?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'list', 'aggregate', 'pandas-groupby']\n",
    "    __TITLE = \"How to group dataframe rows into list in pandas groupby\"\n",
    "    __QUESTION = \"<p>I have a pandas data frame <code>df</code> like:</p><pre><code>a bA 1A 2B 5B 5B 4C 6</code></pre><p>I want to <strong>group by the first column and get second column as lists in rows</strong>:</p><pre><code>A [1,2]B [5,5,4]C [6]</code></pre><p>Is it possible to do something like this using pandas groupby?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22219004/how-to-group-dataframe-rows-into-list-in-pandas-groupby\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc265838",
   "metadata": {},
   "source": [
    "### 63: Improve subplot size/spacing with many subplots in matplotlib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65934423",
   "metadata": {},
   "source": [
    "<p>Very similar to <a href=https://stackoverflow.com/questions/2418125/matplotlib-subplots-adjust-hspace-so-titles-and-xlabels-dont-overlap>this question</a> but with the difference that my figure can be as large as it needs to be.</p><p>I need to generate a whole bunch of vertically-stacked plots in matplotlib. The result will be saved using figsave and viewed on a webpage, so I dont care how tall the final image is as long as the subplots are spaced so they dont overlap. </p><p>No matter how big I allow the figure to be, the subplots always seem to overlap.</p><p>My code currently looks like</p><pre><code>import matplotlib.pyplot as pltimport my_other_moduletitles, x_lists, y_lists = my_other_module.get_data()fig = plt.figure(figsize=(10,60))for i, y_list in enumerate(y_lists):    plt.subplot(len(titles), 1, i)    plt.xlabel(Some X label)    plt.ylabel(Some Y label)    plt.title(titles[i])    plt.plot(x_lists[i],y_list)fig.savefig(out.png, dpi=100)</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'matplotlib', 'seaborn', 'subplot']\n",
    "    __TITLE = \"Improve subplot size/spacing with many subplots in matplotlib\"\n",
    "    __QUESTION = \"<p>Very similar to <a href=https://stackoverflow.com/questions/2418125/matplotlib-subplots-adjust-hspace-so-titles-and-xlabels-dont-overlap>this question</a> but with the difference that my figure can be as large as it needs to be.</p><p>I need to generate a whole bunch of vertically-stacked plots in matplotlib. The result will be saved using figsave and viewed on a webpage, so I dont care how tall the final image is as long as the subplots are spaced so they dont overlap. </p><p>No matter how big I allow the figure to be, the subplots always seem to overlap.</p><p>My code currently looks like</p><pre><code>import matplotlib.pyplot as pltimport my_other_moduletitles, x_lists, y_lists = my_other_module.get_data()fig = plt.figure(figsize=(10,60))for i, y_list in enumerate(y_lists):    plt.subplot(len(titles), 1, i)    plt.xlabel(Some X label)    plt.ylabel(Some Y label)    plt.title(titles[i])    plt.plot(x_lists[i],y_list)fig.savefig(out.png, dpi=100)</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/6541123/improve-subplot-size-spacing-with-many-subplots-in-matplotlib\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1957c",
   "metadata": {},
   "source": [
    "### 64: Selecting/excluding sets of columns in pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9628f3b",
   "metadata": {},
   "source": [
    "<p>I would like to create views or dataframes from an existing dataframe based on column selections.</p><p>For example, I would like to create a dataframe <code>df2</code> from a dataframe <code>df1</code> that holds all columns from it except two of them. I tried doing the following, but it didnt work:</p><pre><code>import numpy as npimport pandas as pd# Create a dataframe with columns A,B,C and Ddf = pd.DataFrame(np.random.randn(100, 4), columns=list(ABCD))# Try to create a second dataframe df2 from df with all columns except B and Dmy_cols = set(df.columns)my_cols.remove(B).remove(D)# This returns an error (unhashable type: set)df2 = df[my_cols]</code></pre><p>What am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and <strong>exclusions</strong> of arbitrary sets of columns from a dataframe?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef22d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Selecting/excluding sets of columns in pandas\"\n",
    "    __QUESTION = \"<p>I would like to create views or dataframes from an existing dataframe based on column selections.</p><p>For example, I would like to create a dataframe <code>df2</code> from a dataframe <code>df1</code> that holds all columns from it except two of them. I tried doing the following, but it didnt work:</p><pre><code>import numpy as npimport pandas as pd# Create a dataframe with columns A,B,C and Ddf = pd.DataFrame(np.random.randn(100, 4), columns=list(ABCD))# Try to create a second dataframe df2 from df with all columns except B and Dmy_cols = set(df.columns)my_cols.remove(B).remove(D)# This returns an error (unhashable type: set)df2 = df[my_cols]</code></pre><p>What am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and <strong>exclusions</strong> of arbitrary sets of columns from a dataframe?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/14940743/selecting-excluding-sets-of-columns-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282ccbd",
   "metadata": {},
   "source": [
    "### 65: Get first row value of a given column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f9e1e",
   "metadata": {},
   "source": [
    "<p>This seems like a ridiculously easy question... but Im not seeing the easy answer I was expecting.</p><p>So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).</p><p>For example, lets say I want to pull the 1.2 value in <code>Btime</code> as a variable.</p><p>Whats the right way to do this?</p><pre class=lang-py prettyprint-override><code>&gt;&gt;&gt; df_test    ATime   X   Y   Z   Btime  C   D   E0    1.2  2  15   2    1.2  12  25  121    1.4  3  12   1    1.3  13  22  112    1.5  1  10   6    1.4  11  20  163    1.6  2   9  10    1.7  12  29  124    1.9  1   1   9    1.9  11  21  195    2.0  0   0   0    2.0   8  10  116    2.4  0   0   0    2.4  10  12  15</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1472a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'indexing', 'head']\n",
    "    __TITLE = \"Get first row value of a given column\"\n",
    "    __QUESTION = \"<p>This seems like a ridiculously easy question... but Im not seeing the easy answer I was expecting.</p><p>So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).</p><p>For example, lets say I want to pull the 1.2 value in <code>Btime</code> as a variable.</p><p>Whats the right way to do this?</p><pre class=lang-py prettyprint-override><code>&gt;&gt;&gt; df_test    ATime   X   Y   Z   Btime  C   D   E0    1.2  2  15   2    1.2  12  25  121    1.4  3  12   1    1.3  13  22  112    1.5  1  10   6    1.4  11  20  163    1.6  2   9  10    1.7  12  29  124    1.9  1   1   9    1.9  11  21  195    2.0  0   0   0    2.0   8  10  116    2.4  0   0   0    2.4  10  12  15</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/25254016/get-first-row-value-of-a-given-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e170e",
   "metadata": {},
   "source": [
    "### 66: Python Pandas: Get index of rows which column matches certain value\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab6df8",
   "metadata": {},
   "source": [
    "<p>Given a DataFrame with a column BoolCol, we want to find the indexes of the DataFrame in which the values for BoolCol == True</p><p>I currently have the iterating way to do it, which works perfectly:</p><pre><code>for i in range(100,3000):    if df.iloc[i][BoolCol]== True:         print i,df.iloc[i][BoolCol]</code></pre><p>But this is not the correct pandas way to do it.After some research, I am currently using this code:</p><pre><code>df[df[BoolCol] == True].index.tolist()</code></pre><p>This one gives me a list of indexes, but they dont match, when I check them by doing:</p><pre><code>df.iloc[i][BoolCol]</code></pre><p>The result is actually False!!</p><p>Which would be the correct Pandas way to do this?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ac16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'indexing', 'pandas']\n",
    "    __TITLE = \"Python Pandas: Get index of rows which column matches certain value\"\n",
    "    __QUESTION = \"<p>Given a DataFrame with a column BoolCol, we want to find the indexes of the DataFrame in which the values for BoolCol == True</p><p>I currently have the iterating way to do it, which works perfectly:</p><pre><code>for i in range(100,3000):    if df.iloc[i][BoolCol]== True:         print i,df.iloc[i][BoolCol]</code></pre><p>But this is not the correct pandas way to do it.After some research, I am currently using this code:</p><pre><code>df[df[BoolCol] == True].index.tolist()</code></pre><p>This one gives me a list of indexes, but they dont match, when I check them by doing:</p><pre><code>df.iloc[i][BoolCol]</code></pre><p>The result is actually False!!</p><p>Which would be the correct Pandas way to do this?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-which-column-matches-certain-value\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb264c",
   "metadata": {},
   "source": [
    "### 67: Pandas conditional creation of a series/dataframe column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b5ffc",
   "metadata": {},
   "source": [
    "<p>How do I add a <code>color</code> column to the following dataframe so that <code>color=green</code> if <code>Set == Z</code>, and <code>color=red</code> otherwise?</p><pre><code>    Type       Set1    A          Z2    B          Z           3    B          X4    C          Y</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'numpy', 'dataframe']\n",
    "    __TITLE = \"Pandas conditional creation of a series/dataframe column\"\n",
    "    __QUESTION = \"<p>How do I add a <code>color</code> column to the following dataframe so that <code>color=green</code> if <code>Set == Z</code>, and <code>color=red</code> otherwise?</p><pre><code>    Type       Set1    A          Z2    B          Z           3    B          X4    C          Y</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cbcc1e",
   "metadata": {},
   "source": [
    "### 68: Sorting columns in pandas dataframe based on column name\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b20bb",
   "metadata": {},
   "source": [
    "<p>I have a <code>dataframe</code> with over 200 columns. The issue is as they were generated the order is</p><pre><code>[Q1.3,Q6.1,Q1.2,Q1.1,......]</code></pre><p>I need to sort the columns as follows:</p><pre><code>[Q1.1,Q1.2,Q1.3,.....Q6.1,......]</code></pre><p>Is there some way for me to do this within Python?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b522fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"Sorting columns in pandas dataframe based on column name\"\n",
    "    __QUESTION = \"<p>I have a <code>dataframe</code> with over 200 columns. The issue is as they were generated the order is</p><pre><code>[Q1.3,Q6.1,Q1.2,Q1.1,......]</code></pre><p>I need to sort the columns as follows:</p><pre><code>[Q1.1,Q1.2,Q1.3,.....Q6.1,......]</code></pre><p>Is there some way for me to do this within Python?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11067027/sorting-columns-in-pandas-dataframe-based-on-column-name\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a61f4",
   "metadata": {},
   "source": [
    "### 69: Count the frequency that a value occurs in a dataframe column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d3650",
   "metadata": {},
   "source": [
    "<p>I have a dataset</p><pre><code>categorycat acat bcat a</code></pre><p>Id like to be able to return something like (showing unique values and frequency)</p><pre><code>category   freq cat a       2cat b       1</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec622a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'frequency']\n",
    "    __TITLE = \"Count the frequency that a value occurs in a dataframe column\"\n",
    "    __QUESTION = \"<p>I have a dataset</p><pre><code>categorycat acat bcat a</code></pre><p>Id like to be able to return something like (showing unique values and frequency)</p><pre><code>category   freq cat a       2cat b       1</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22391433/count-the-frequency-that-a-value-occurs-in-a-dataframe-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04806c",
   "metadata": {},
   "source": [
    "### 70: What does `ValueError: cannot reindex from a duplicate axis` mean?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e94ba",
   "metadata": {},
   "source": [
    "<p>I am getting a <code>ValueError: cannot reindex from a duplicate axis</code> when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.</p><p>Here is my session inside of <code>ipdb</code> trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create <code>sum</code> index for sum of all columns I am getting <code>ValueError: cannot reindex from a duplicate axis</code> error. I created a small DataFrame with the same characteristics, but was not able to reproduce the problem, what could I be missing?</p><p>I dont really understand what <code>ValueError: cannot reindex from a duplicate axis</code>means, what does this error message mean? Maybe this will help me diagnose the problem, and this is most answerable part of my question.</p><pre><code>ipdb&gt; type(affinity_matrix)&lt;class pandas.core.frame.DataFrame&gt;ipdb&gt; affinity_matrix.shape(333, 10)ipdb&gt; affinity_matrix.columnsInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype=int64)ipdb&gt; affinity_matrix.indexIndex([u001, u002, u003, u004, u005, u008, u009, u010, u011, u014, u015, u016, u018, u020, u021, u022, u024, u025, u026, u027, u028, u029, u030, u032, u033, u034, u035, u036, u039, u040, u041, u042, u043, u044, u045, u047, u047, u048, u050, u053, u054, u055, u056, u057, u058, u059, u060, u061, u062, u063, u065, u067, u068, u069, u070, u071, u072, u073, u074, u075, u076, u077, u078, u080, u082, u083, u084, u085, u086, u089, u090, u091, u092, u093, u094, u095, u096, u097, u098, u100, u101, u103, u104, u105, u106, u107, u108, u109, u110, u111, u112, u113, u114, u115, u116, u117, u118, u119, u121, u122, ...], dtype=object)ipdb&gt; affinity_matrix.values.dtypedtype(float64)ipdb&gt; sums in affinity_matrix.indexFalse</code></pre><p>Here is the error:</p><pre><code>ipdb&gt; affinity_matrix.loc[sums] = affinity_matrix.sum(axis=0)*** ValueError: cannot reindex from a duplicate axis</code></pre><p>I tried to reproduce this with a simple example, but I failed</p><pre><code>In [32]: import pandas as pdIn [33]: import numpy as npIn [34]: a = np.arange(35).reshape(5,7)In [35]: df = pd.DataFrame(a, [x, y, u, z, w], range(10, 17))In [36]: df.values.dtypeOut[36]: dtype(int64)In [37]: df.loc[sums] = df.sum(axis=0)In [38]: dfOut[38]:       10  11  12  13  14  15   16x      0   1   2   3   4   5    6y      7   8   9  10  11  12   13u     14  15  16  17  18  19   20z     21  22  23  24  25  26   27w     28  29  30  31  32  33   34sums  70  75  80  85  90  95  100</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"What does `ValueError: cannot reindex from a duplicate axis` mean?\"\n",
    "    __QUESTION = \"<p>I am getting a <code>ValueError: cannot reindex from a duplicate axis</code> when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.</p><p>Here is my session inside of <code>ipdb</code> trace. I have a DataFrame with string index, and integer columns, float values. However when I try to create <code>sum</code> index for sum of all columns I am getting <code>ValueError: cannot reindex from a duplicate axis</code> error. I created a small DataFrame with the same characteristics, but was not able to reproduce the problem, what could I be missing?</p><p>I dont really understand what <code>ValueError: cannot reindex from a duplicate axis</code>means, what does this error message mean? Maybe this will help me diagnose the problem, and this is most answerable part of my question.</p><pre><code>ipdb&gt; type(affinity_matrix)&lt;class pandas.core.frame.DataFrame&gt;ipdb&gt; affinity_matrix.shape(333, 10)ipdb&gt; affinity_matrix.columnsInt64Index([9315684, 9315597, 9316591, 9320520, 9321163, 9320615, 9321187, 9319487, 9319467, 9320484], dtype=int64)ipdb&gt; affinity_matrix.indexIndex([u001, u002, u003, u004, u005, u008, u009, u010, u011, u014, u015, u016, u018, u020, u021, u022, u024, u025, u026, u027, u028, u029, u030, u032, u033, u034, u035, u036, u039, u040, u041, u042, u043, u044, u045, u047, u047, u048, u050, u053, u054, u055, u056, u057, u058, u059, u060, u061, u062, u063, u065, u067, u068, u069, u070, u071, u072, u073, u074, u075, u076, u077, u078, u080, u082, u083, u084, u085, u086, u089, u090, u091, u092, u093, u094, u095, u096, u097, u098, u100, u101, u103, u104, u105, u106, u107, u108, u109, u110, u111, u112, u113, u114, u115, u116, u117, u118, u119, u121, u122, ...], dtype=object)ipdb&gt; affinity_matrix.values.dtypedtype(float64)ipdb&gt; sums in affinity_matrix.indexFalse</code></pre><p>Here is the error:</p><pre><code>ipdb&gt; affinity_matrix.loc[sums] = affinity_matrix.sum(axis=0)*** ValueError: cannot reindex from a duplicate axis</code></pre><p>I tried to reproduce this with a simple example, but I failed</p><pre><code>In [32]: import pandas as pdIn [33]: import numpy as npIn [34]: a = np.arange(35).reshape(5,7)In [35]: df = pd.DataFrame(a, [x, y, u, z, w], range(10, 17))In [36]: df.values.dtypeOut[36]: dtype(int64)In [37]: df.loc[sums] = df.sum(axis=0)In [38]: dfOut[38]:       10  11  12  13  14  15   16x      0   1   2   3   4   5    6y      7   8   9  10  11  12   13u     14  15  16  17  18  19   20z     21  22  23  24  25  26   27w     28  29  30  31  32  33   34sums  70  75  80  85  90  95  100</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/27236275/what-does-valueerror-cannot-reindex-from-a-duplicate-axis-mean\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35208ed4",
   "metadata": {},
   "source": [
    "### 71: Convert DataFrame column type from string to datetime\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3602",
   "metadata": {},
   "source": [
    "<p>How can I convert a DataFrame column of strings (in <strong><em>dd/mm/yyyy</em></strong> format) to datetimes?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'datetime-format', 'python-datetime']\n",
    "    __TITLE = \"Convert DataFrame column type from string to datetime\"\n",
    "    __QUESTION = \"<p>How can I convert a DataFrame column of strings (in <strong><em>dd/mm/yyyy</em></strong> format) to datetimes?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17134716/convert-dataframe-column-type-from-string-to-datetime\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca28ed9",
   "metadata": {},
   "source": [
    "### 72: Create Pandas DataFrame from a string\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701bfbe",
   "metadata": {},
   "source": [
    "<p>In order to test some functionality I would like to create a <code>DataFrame</code> from a string. Lets say my test data looks like:</p><pre><code>TESTDATA=col1;col2;col31;4.4;992;4.5;2003;4.7;654;3.2;140</code></pre><p>What is the simplest way to read that data into a Pandas <code>DataFrame</code>?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'string', 'pandas', 'csv', 'csv-import']\n",
    "    __TITLE = \"Create Pandas DataFrame from a string\"\n",
    "    __QUESTION = \"<p>In order to test some functionality I would like to create a <code>DataFrame</code> from a string. Lets say my test data looks like:</p><pre><code>TESTDATA=col1;col2;col31;4.4;992;4.5;2003;4.7;654;3.2;140</code></pre><p>What is the simplest way to read that data into a Pandas <code>DataFrame</code>?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22604564/create-pandas-dataframe-from-a-string\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3556be",
   "metadata": {},
   "source": [
    "### 73: How to add an empty column to a dataframe?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20953d0",
   "metadata": {},
   "source": [
    "<p>Whats the easiest way to add an empty column to a pandas <code>DataFrame</code> object?  The best Ive stumbled upon is something like</p><pre><code>df[foo] = df.apply(lambda _: , axis=1)</code></pre><p>Is there a less perverse method?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"How to add an empty column to a dataframe?\"\n",
    "    __QUESTION = \"<p>Whats the easiest way to add an empty column to a pandas <code>DataFrame</code> object?  The best Ive stumbled upon is something like</p><pre><code>df[foo] = df.apply(lambda _: , axis=1)</code></pre><p>Is there a less perverse method?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be2e7a",
   "metadata": {},
   "source": [
    "### 74: How to sort a dataFrame in python pandas by two or more columns?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9460d87",
   "metadata": {},
   "source": [
    "<p>Suppose I have a dataframe with columns <code>a</code>, <code>b</code> and <code>c</code>, I want to sort the dataframe by column <code>b</code> in ascending order, and by column <code>c</code> in descending order, how do I do this?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb31945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'python-2.7', 'sorting', 'data-analysis']\n",
    "    __TITLE = \"How to sort a dataFrame in python pandas by two or more columns?\"\n",
    "    __QUESTION = \"<p>Suppose I have a dataframe with columns <code>a</code>, <code>b</code> and <code>c</code>, I want to sort the dataframe by column <code>b</code> in ascending order, and by column <code>c</code> in descending order, how do I do this?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17141558/how-to-sort-a-dataframe-in-python-pandas-by-two-or-more-columns\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d0b7a",
   "metadata": {},
   "source": [
    "### 75: How to reversibly store and load a Pandas dataframe to/from disk\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ebd37",
   "metadata": {},
   "source": [
    "<p>Right now Im importing a fairly large <code>CSV</code> as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I dont have to spend all that time waiting for the script to run?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"How to reversibly store and load a Pandas dataframe to/from disk\"\n",
    "    __QUESTION = \"<p>Right now Im importing a fairly large <code>CSV</code> as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I dont have to spend all that time waiting for the script to run?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17098654/how-to-reversibly-store-and-load-a-pandas-dataframe-to-from-disk\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3459a95",
   "metadata": {},
   "source": [
    "### 76: Convert Pandas Column to DateTime\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f2f12",
   "metadata": {},
   "source": [
    "<p>I have one field in a pandas DataFrame that was imported as string format. It should be a datetime variable.How do I convert it to a datetime column and then filter based on date.</p><p>Example:</p><ul><li>DataFrame Name: <strong>raw_data</strong>    </li><li>Column Name: <strong>Mycol</strong>    </li><li>ValueFormat in Column: <strong>05SEP2014:00:00:00.000</strong></li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'datetime', 'pandas']\n",
    "    __TITLE = \"Convert Pandas Column to DateTime\"\n",
    "    __QUESTION = \"<p>I have one field in a pandas DataFrame that was imported as string format. It should be a datetime variable.How do I convert it to a datetime column and then filter based on date.</p><p>Example:</p><ul><li>DataFrame Name: <strong>raw_data</strong>    </li><li>Column Name: <strong>Mycol</strong>    </li><li>ValueFormat in Column: <strong>05SEP2014:00:00:00.000</strong></li></ul>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28e2c7",
   "metadata": {},
   "source": [
    "### 77: How to draw vertical lines on a given plot\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84e4ce",
   "metadata": {},
   "source": [
    "<p>Given a plot of a signal in time representation, how can I draw lines marking the corresponding time index?</p><p>Specifically, given a signal plot with a time index ranging from 0 to 2.6 (seconds), I want to draw vertical red lines indicating the corresponding time index for the list <code>[0.22058956, 0.33088437, 2.20589566]</code>. How can I do it?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbe11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'matplotlib', 'seaborn']\n",
    "    __TITLE = \"How to draw vertical lines on a given plot\"\n",
    "    __QUESTION = \"<p>Given a plot of a signal in time representation, how can I draw lines marking the corresponding time index?</p><p>Specifically, given a signal plot with a time index ranging from 0 to 2.6 (seconds), I want to draw vertical red lines indicating the corresponding time index for the list <code>[0.22058956, 0.33088437, 2.20589566]</code>. How can I do it?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/24988448/how-to-draw-vertical-lines-on-a-given-plot\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7ec87",
   "metadata": {},
   "source": [
    "### 78: Converting between datetime, Timestamp and datetime64\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c5cf1",
   "metadata": {},
   "source": [
    "<p>How do I convert a <code>numpy.datetime64</code> object to a <code>datetime.datetime</code> (or <code>Timestamp</code>)?</p><p>In the following code, I create a datetime, timestamp and datetime64 objects.</p><pre><code>import datetimeimport numpy as npimport pandas as pddt = datetime.datetime(2012, 5, 1)# A strange way to extract a Timestamp object, theres surely a better way?ts = pd.DatetimeIndex([dt])[0]dt64 = np.datetime64(dt)In [7]: dtOut[7]: datetime.datetime(2012, 5, 1, 0, 0)In [8]: tsOut[8]: &lt;Timestamp: 2012-05-01 00:00:00&gt;In [9]: dt64Out[9]: numpy.datetime64(2012-05-01T01:00:00.000000+0100)</code></pre><p><em>Note: its easy to get the datetime from the Timestamp:</em></p><pre><code>In [10]: ts.to_datetime()Out[10]: datetime.datetime(2012, 5, 1, 0, 0)</code></pre><p>But how do we extract the <code>datetime</code> or <code>Timestamp</code> from a <code>numpy.datetime64</code> (<code>dt64</code>)?</p><p>.</p><p>Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:</p><pre><code>dt64 = numpy.datetime64(2002-06-28T01:00:00.000000000+0100)</code></pre><p>which should be <code>datetime.datetime(2002, 6, 28, 1, 0)</code>, and not a long (!) (<code>1025222400000000000L</code>)...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fab326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'datetime', 'numpy', 'pandas']\n",
    "    __TITLE = \"Converting between datetime, Timestamp and datetime64\"\n",
    "    __QUESTION = \"<p>How do I convert a <code>numpy.datetime64</code> object to a <code>datetime.datetime</code> (or <code>Timestamp</code>)?</p><p>In the following code, I create a datetime, timestamp and datetime64 objects.</p><pre><code>import datetimeimport numpy as npimport pandas as pddt = datetime.datetime(2012, 5, 1)# A strange way to extract a Timestamp object, theres surely a better way?ts = pd.DatetimeIndex([dt])[0]dt64 = np.datetime64(dt)In [7]: dtOut[7]: datetime.datetime(2012, 5, 1, 0, 0)In [8]: tsOut[8]: &lt;Timestamp: 2012-05-01 00:00:00&gt;In [9]: dt64Out[9]: numpy.datetime64(2012-05-01T01:00:00.000000+0100)</code></pre><p><em>Note: its easy to get the datetime from the Timestamp:</em></p><pre><code>In [10]: ts.to_datetime()Out[10]: datetime.datetime(2012, 5, 1, 0, 0)</code></pre><p>But how do we extract the <code>datetime</code> or <code>Timestamp</code> from a <code>numpy.datetime64</code> (<code>dt64</code>)?</p><p>.</p><p>Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:</p><pre><code>dt64 = numpy.datetime64(2002-06-28T01:00:00.000000000+0100)</code></pre><p>which should be <code>datetime.datetime(2002, 6, 28, 1, 0)</code>, and not a long (!) (<code>1025222400000000000L</code>)...</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca4908",
   "metadata": {},
   "source": [
    "### 79: Normalize columns of pandas data frame\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce44d4",
   "metadata": {},
   "source": [
    "<p>I have a dataframe in pandas where each column has different value range. For example:</p><p>df:</p><pre><code>A     B   C1000  10  0.5765   5   0.35800   7   0.09</code></pre><p>Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?</p><p>My desired output is:</p><pre><code>A     B    C1     1    10.765 0.5  0.70.8   0.7  0.18(which is 0.09/0.5)</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'normalize']\n",
    "    __TITLE = \"Normalize columns of pandas data frame\"\n",
    "    __QUESTION = \"<p>I have a dataframe in pandas where each column has different value range. For example:</p><p>df:</p><pre><code>A     B   C1000  10  0.5765   5   0.35800   7   0.09</code></pre><p>Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?</p><p>My desired output is:</p><pre><code>A     B    C1     1    10.765 0.5  0.70.8   0.7  0.18(which is 0.09/0.5)</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a6e7b",
   "metadata": {},
   "source": [
    "### 80: pandas: filter rows of DataFrame with operator chaining\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439d59e",
   "metadata": {},
   "source": [
    "<p>Most operations in <code>pandas</code> can be accomplished with operator chaining (<code>groupby</code>, <code>aggregate</code>, <code>apply</code>, etc), but the only way Ive found to filter rows is via normal bracket indexing</p><pre><code>df_filtered = df[df[column] == value]</code></pre><p>This is unappealing as it requires I assign <code>df</code> to a variable before being able to filter on its values.  Is there something more like the following?</p><pre><code>df_filtered = df.mask(lambda x: x[column] == value)</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"pandas: filter rows of DataFrame with operator chaining\"\n",
    "    __QUESTION = \"<p>Most operations in <code>pandas</code> can be accomplished with operator chaining (<code>groupby</code>, <code>aggregate</code>, <code>apply</code>, etc), but the only way Ive found to filter rows is via normal bracket indexing</p><pre><code>df_filtered = df[df[column] == value]</code></pre><p>This is unappealing as it requires I assign <code>df</code> to a variable before being able to filter on its values.  Is there something more like the following?</p><pre><code>df_filtered = df.mask(lambda x: x[column] == value)</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931b17a",
   "metadata": {},
   "source": [
    "### 81: Get the row(s) which have the max value in groups using groupby\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f8c39",
   "metadata": {},
   "source": [
    "<p>How do I find all rows in a pandas DataFrame which have the max value for <code>count</code> column, after grouping by <code>[Sp,Mt]</code> columns?</p><p><strong>Example 1:</strong> the following DataFrame, which I group by <code>[Sp,Mt]</code>:</p><pre><code>   Sp   Mt Value   count0  MM1  S1   a     **3**1  MM1  S1   n       22  MM1  S3   cb    **5**3  MM2  S3   mk    **8**4  MM2  S4   bg    **10**5  MM2  S4   dgd     16  MM4  S2   rd      27  MM4  S2   cb      28  MM4  S2   uyi   **7**</code></pre><p>Expected output: get the result rows whose count is max in each group, like:</p><pre><code>0  MM1  S1   a      **3**2  MM1  S3   cb     **5**3  MM2  S3   mk     **8**4  MM2  S4   bg     **10** 8  MM4  S2   uyi    **7**</code></pre><p><strong>Example 2:</strong> this DataFrame, which I group by <code>[Sp,Mt]</code>:</p><pre><code>   Sp   Mt   Value  count4  MM2  S4   bg     105  MM2  S4   dgd    16  MM4  S2   rd     27  MM4  S2   cb     88  MM4  S2   uyi    8</code></pre><p>For the above example, I want to get <strong>all</strong> the rows where <code>count</code> equals max, in each group e.g:</p><pre><code>MM2  S4   bg     10MM4  S2   cb     8MM4  S2   uyi    8</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f471c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'max', 'pandas-groupby']\n",
    "    __TITLE = \"Get the row(s) which have the max value in groups using groupby\"\n",
    "    __QUESTION = \"<p>How do I find all rows in a pandas DataFrame which have the max value for <code>count</code> column, after grouping by <code>[Sp,Mt]</code> columns?</p><p><strong>Example 1:</strong> the following DataFrame, which I group by <code>[Sp,Mt]</code>:</p><pre><code>   Sp   Mt Value   count0  MM1  S1   a     **3**1  MM1  S1   n       22  MM1  S3   cb    **5**3  MM2  S3   mk    **8**4  MM2  S4   bg    **10**5  MM2  S4   dgd     16  MM4  S2   rd      27  MM4  S2   cb      28  MM4  S2   uyi   **7**</code></pre><p>Expected output: get the result rows whose count is max in each group, like:</p><pre><code>0  MM1  S1   a      **3**2  MM1  S3   cb     **5**3  MM2  S3   mk     **8**4  MM2  S4   bg     **10** 8  MM4  S2   uyi    **7**</code></pre><p><strong>Example 2:</strong> this DataFrame, which I group by <code>[Sp,Mt]</code>:</p><pre><code>   Sp   Mt   Value  count4  MM2  S4   bg     105  MM2  S4   dgd    16  MM4  S2   rd     27  MM4  S2   cb     88  MM4  S2   uyi    8</code></pre><p>For the above example, I want to get <strong>all</strong> the rows where <code>count</code> equals max, in each group e.g:</p><pre><code>MM2  S4   bg     10MM4  S2   cb     8MM4  S2   uyi    8</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15647fb",
   "metadata": {},
   "source": [
    "### 82: How to add pandas data to an existing csv file?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551bd80c",
   "metadata": {},
   "source": [
    "<p>I want to know if it is possible to use the pandas <code>to_csv()</code> function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'csv', 'dataframe']\n",
    "    __TITLE = \"How to add pandas data to an existing csv file?\"\n",
    "    __QUESTION = \"<p>I want to know if it is possible to use the pandas <code>to_csv()</code> function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data. </p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/17530542/how-to-add-pandas-data-to-an-existing-csv-file\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204ae76",
   "metadata": {},
   "source": [
    "### 83: Remove pandas rows with duplicate indices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819be282",
   "metadata": {},
   "source": [
    "<p>How to remove rows with duplicate index values?</p><p>In the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.</p><p>Im reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like:</p><pre><code>                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPressDate                                                                                      2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.312001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.302001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.302001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.302001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28</code></pre><p>Example of a duplicate case:</p><pre><code>import pandas import datetimestartdate = datetime.datetime(2001, 1, 1, 0, 0)enddate = datetime.datetime(2001, 1, 1, 5, 0)index = pandas.DatetimeIndex(start=startdate, end=enddate, freq=H)data1 = {A : range(6), B : range(6)}data2 = {A : [20, -30, 40], B : [-50, 60, -70]}df1 = pandas.DataFrame(data=data1, index=index)df2 = pandas.DataFrame(data=data2, index=index[:3])df3 = df2.append(df1)df3                       A   B2001-01-01 00:00:00   20 -502001-01-01 01:00:00  -30  602001-01-01 02:00:00   40 -702001-01-01 03:00:00    3   32001-01-01 04:00:00    4   42001-01-01 05:00:00    5   52001-01-01 00:00:00    0   02001-01-01 01:00:00    1   12001-01-01 02:00:00    2   2</code></pre><p>And so I need <code>df3</code> to eventually become:</p><pre><code>                       A   B2001-01-01 00:00:00    0   02001-01-01 01:00:00    1   12001-01-01 02:00:00    2   22001-01-01 03:00:00    3   32001-01-01 04:00:00    4   42001-01-01 05:00:00    5   5</code></pre><p>I thought that adding a column of row numbers (<code>df3[rownum] = range(df3.shape[0])</code>) would help me select the bottom-most row for any value of the <code>DatetimeIndex</code>, but I am stuck on figuring out the <code>group_by</code> or <code>pivot</code> (or ???) statements to make that work.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'duplicates']\n",
    "    __TITLE = \"Remove pandas rows with duplicate indices\"\n",
    "    __QUESTION = \"<p>How to remove rows with duplicate index values?</p><p>In the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.</p><p>Im reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like:</p><pre><code>                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPressDate                                                                                      2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.312001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.302001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.302001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.302001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28</code></pre><p>Example of a duplicate case:</p><pre><code>import pandas import datetimestartdate = datetime.datetime(2001, 1, 1, 0, 0)enddate = datetime.datetime(2001, 1, 1, 5, 0)index = pandas.DatetimeIndex(start=startdate, end=enddate, freq=H)data1 = {A : range(6), B : range(6)}data2 = {A : [20, -30, 40], B : [-50, 60, -70]}df1 = pandas.DataFrame(data=data1, index=index)df2 = pandas.DataFrame(data=data2, index=index[:3])df3 = df2.append(df1)df3                       A   B2001-01-01 00:00:00   20 -502001-01-01 01:00:00  -30  602001-01-01 02:00:00   40 -702001-01-01 03:00:00    3   32001-01-01 04:00:00    4   42001-01-01 05:00:00    5   52001-01-01 00:00:00    0   02001-01-01 01:00:00    1   12001-01-01 02:00:00    2   2</code></pre><p>And so I need <code>df3</code> to eventually become:</p><pre><code>                       A   B2001-01-01 00:00:00    0   02001-01-01 01:00:00    1   12001-01-01 02:00:00    2   22001-01-01 03:00:00    3   32001-01-01 04:00:00    4   42001-01-01 05:00:00    5   5</code></pre><p>I thought that adding a column of row numbers (<code>df3[rownum] = range(df3.shape[0])</code>) would help me select the bottom-most row for any value of the <code>DatetimeIndex</code>, but I am stuck on figuring out the <code>group_by</code> or <code>pivot</code> (or ???) statements to make that work.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13035764/remove-pandas-rows-with-duplicate-indices\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c78ed3",
   "metadata": {},
   "source": [
    "### 84: What is the most efficient way to loop through dataframes with pandas?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f89ec2f",
   "metadata": {},
   "source": [
    "<p>I want to perform my own complex operations on financial data in dataframes in a sequential manner.</p><p>For example I am using the following MSFT CSV file taken from <a href=http://finance.yahoo.com/q/hp?s=MSFT>Yahoo Finance</a>:</p><pre><code>Date,Open,High,Low,Close,Volume,Adj Close2011-10-19,27.37,27.47,27.01,27.13,42880000,27.132011-10-18,26.94,27.40,26.80,27.31,52487900,27.312011-10-17,27.11,27.42,26.85,26.98,39433400,26.982011-10-14,27.31,27.50,27.02,27.27,50947700,27.27....</code></pre><p>I then do the following:</p><pre><code>#!/usr/bin/env pythonfrom pandas import *df = read_csv(table.csv)for i, row in enumerate(df.values):    date = df.index[i]    open, high, low, close, adjclose = row    #now perform analysis on open/close based on date, etc..</code></pre><p>Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? <code>df.iteritems</code> unfortunately only iterates column by column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'performance', 'dataframe', 'for-loop']\n",
    "    __TITLE = \"What is the most efficient way to loop through dataframes with pandas?\"\n",
    "    __QUESTION = \"<p>I want to perform my own complex operations on financial data in dataframes in a sequential manner.</p><p>For example I am using the following MSFT CSV file taken from <a href=http://finance.yahoo.com/q/hp?s=MSFT>Yahoo Finance</a>:</p><pre><code>Date,Open,High,Low,Close,Volume,Adj Close2011-10-19,27.37,27.47,27.01,27.13,42880000,27.132011-10-18,26.94,27.40,26.80,27.31,52487900,27.312011-10-17,27.11,27.42,26.85,26.98,39433400,26.982011-10-14,27.31,27.50,27.02,27.27,50947700,27.27....</code></pre><p>I then do the following:</p><pre><code>#!/usr/bin/env pythonfrom pandas import *df = read_csv(table.csv)for i, row in enumerate(df.values):    date = df.index[i]    open, high, low, close, adjclose = row    #now perform analysis on open/close based on date, etc..</code></pre><p>Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? <code>df.iteritems</code> unfortunately only iterates column by column.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad4b58",
   "metadata": {},
   "source": [
    "### 85: Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3e2b1",
   "metadata": {},
   "source": [
    "<p>I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:</p><pre><code>data = array([[,Col1,Col2],[Row1,1,2],[Row2,3,4]])</code></pre><p>Id like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values</p><p>I can specify the index as follows:</p><pre><code>df = pd.DataFrame(data,index=data[:,0]),</code></pre><p>however I am unsure how to best assign column headers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbf1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'numpy']\n",
    "    __TITLE = \"Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?\"\n",
    "    __QUESTION = \"<p>I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:</p><pre><code>data = array([[,Col1,Col2],[Row1,1,2],[Row2,3,4]])</code></pre><p>Id like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values</p><p>I can specify the index as follows:</p><pre><code>df = pd.DataFrame(data,index=data[:,0]),</code></pre><p>however I am unsure how to best assign column headers.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/20763012/creating-a-pandas-dataframe-from-a-numpy-array-how-do-i-specify-the-index-colum\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e61420",
   "metadata": {},
   "source": [
    "### 86: How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1ce2d",
   "metadata": {},
   "source": [
    "<p>I converted a Pandas dataframe to an HTML output using the <code>DataFrame.to_html</code> function. When I save this to a separate HTML file, the file shows truncated output.</p><p>For example, in my TEXT column,</p><p><code>df.head(1)</code> will show</p><p><em>The film was an excellent effort...</em></p><p>instead of</p><p><em>The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.</em></p><p>This rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet.</p><p>How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the HTML version of the information? I would imagine that the HTML table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the <code>DataFrame.to_html</code> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ec432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'html', 'pandas']\n",
    "    __TITLE = \"How can I display full (non-truncated) dataframe information in HTML when converting from Pandas dataframe to HTML?\"\n",
    "    __QUESTION = \"<p>I converted a Pandas dataframe to an HTML output using the <code>DataFrame.to_html</code> function. When I save this to a separate HTML file, the file shows truncated output.</p><p>For example, in my TEXT column,</p><p><code>df.head(1)</code> will show</p><p><em>The film was an excellent effort...</em></p><p>instead of</p><p><em>The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.</em></p><p>This rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet.</p><p>How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the HTML version of the information? I would imagine that the HTML table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the <code>DataFrame.to_html</code> function.</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/25351968/how-can-i-display-full-non-truncated-dataframe-information-in-html-when-conver\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d801a0d",
   "metadata": {},
   "source": [
    "### 87: Pandas &#39;count(distinct)&#39; equivalent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d399488",
   "metadata": {},
   "source": [
    "<p>I am using Pandas as a database substitute as I have multiple databases (<a href=https://en.wikipedia.org/wiki/Oracle_Database rel=noreferrer>Oracle</a>, <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server rel=noreferrer>SQL Server</a>, etc.), and I am unable to make a sequence of commands to a SQL equivalent.</p><p>I have a table loaded in a DataFrame with some columns:</p><pre><code>YEARMONTH, CLIENTCODE, SIZE, etc., etc.</code></pre><p>In SQL, to count the amount of different clients per year would be:</p><pre><code>SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;</code></pre><p>And the result would be</p><pre><code>201301    5000201302    13245</code></pre><p>How can I do that in Pandas?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'count', 'group-by', 'distinct']\n",
    "    __TITLE = \"Pandas &#39;count(distinct)&#39; equivalent\"\n",
    "    __QUESTION = \"<p>I am using Pandas as a database substitute as I have multiple databases (<a href=https://en.wikipedia.org/wiki/Oracle_Database rel=noreferrer>Oracle</a>, <a href=https://en.wikipedia.org/wiki/Microsoft_SQL_Server rel=noreferrer>SQL Server</a>, etc.), and I am unable to make a sequence of commands to a SQL equivalent.</p><p>I have a table loaded in a DataFrame with some columns:</p><pre><code>YEARMONTH, CLIENTCODE, SIZE, etc., etc.</code></pre><p>In SQL, to count the amount of different clients per year would be:</p><pre><code>SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;</code></pre><p>And the result would be</p><pre><code>201301    5000201302    13245</code></pre><p>How can I do that in Pandas?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/15411158/pandas-countdistinct-equivalent\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b48dd",
   "metadata": {},
   "source": [
    "### 88: What does axis in pandas mean?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692322e6",
   "metadata": {},
   "source": [
    "<p>Here is my code to generate a dataframe:</p><pre><code>import pandas as pdimport numpy as npdff = pd.DataFrame(np.random.randn(1,2),columns=list(AB))</code></pre><p>then I got the dataframe:</p><pre><code>+------------+---------+--------+|            |  A      |  B     |+------------+---------+---------|      0     | 0.626386| 1.52325|+------------+---------+--------+</code></pre><p>When I type the commmand :</p><pre><code>dff.mean(axis=1)</code></pre><p>I got :</p><pre><code>0    1.074821dtype: float64</code></pre><p>According to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be</p><pre><code>A    0.626386B    1.523255dtype: float64</code></pre><p>So here is my question: what does axis in pandas mean?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4770ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'numpy', 'dataframe']\n",
    "    __TITLE = \"What does axis in pandas mean?\"\n",
    "    __QUESTION = \"<p>Here is my code to generate a dataframe:</p><pre><code>import pandas as pdimport numpy as npdff = pd.DataFrame(np.random.randn(1,2),columns=list(AB))</code></pre><p>then I got the dataframe:</p><pre><code>+------------+---------+--------+|            |  A      |  B     |+------------+---------+---------|      0     | 0.626386| 1.52325|+------------+---------+--------+</code></pre><p>When I type the commmand :</p><pre><code>dff.mean(axis=1)</code></pre><p>I got :</p><pre><code>0    1.074821dtype: float64</code></pre><p>According to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be</p><pre><code>A    0.626386B    1.523255dtype: float64</code></pre><p>So here is my question: what does axis in pandas mean?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22149584/what-does-axis-in-pandas-mean\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c7b03",
   "metadata": {},
   "source": [
    "### 89: Pandas index column title or name\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9918c",
   "metadata": {},
   "source": [
    "<p>How do I get the index column name in python pandas?  Heres an example dataframe:</p><pre><code>             Column 1Index Title          Apples              1Oranges             2Puppies             3Ducks               4  </code></pre><p>What Im trying to do is get/set the dataframe index title.  Here is what i tried:</p><pre><code>import pandas as pddata = {Column 1     : [1., 2., 3., 4.],        Index Title  : [Apples, Oranges, Puppies, Ducks]}df = pd.DataFrame(data)df.index = df[Index Title]del df[Index Title]print df</code></pre><p>Anyone know how to do this? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a026622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'columnname']\n",
    "    __TITLE = \"Pandas index column title or name\"\n",
    "    __QUESTION = \"<p>How do I get the index column name in python pandas?  Heres an example dataframe:</p><pre><code>             Column 1Index Title          Apples              1Oranges             2Puppies             3Ducks               4  </code></pre><p>What Im trying to do is get/set the dataframe index title.  Here is what i tried:</p><pre><code>import pandas as pddata = {Column 1     : [1., 2., 3., 4.],        Index Title  : [Apples, Oranges, Puppies, Ducks]}df = pd.DataFrame(data)df.index = df[Index Title]del df[Index Title]print df</code></pre><p>Anyone know how to do this? </p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18022845/pandas-index-column-title-or-name\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55218699",
   "metadata": {},
   "source": [
    "### 90: How to invert the x or y axis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a8b02",
   "metadata": {},
   "source": [
    "<p>I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.</p><pre><code>points = [(10,5), (5,11), (24,13), (7,8)]    x_arr = []y_arr = []for x,y in points:    x_arr.append(x)    y_arr.append(y)plt.scatter(x_arr,y_arr)</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'matplotlib', 'seaborn']\n",
    "    __TITLE = \"How to invert the x or y axis\"\n",
    "    __QUESTION = \"<p>I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.</p><pre><code>points = [(10,5), (5,11), (24,13), (7,8)]    x_arr = []y_arr = []for x,y in points:    x_arr.append(x)    y_arr.append(y)plt.scatter(x_arr,y_arr)</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/2051744/how-to-invert-the-x-or-y-axis\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b7e9f",
   "metadata": {},
   "source": [
    "### 91: Combining two Series into a DataFrame in pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e97b9",
   "metadata": {},
   "source": [
    "<p>I have two Series <code>s1</code> and <code>s2</code> with the same (non-consecutive) indices. How do I combine <code>s1</code> and <code>s2</code> to being two columns in a DataFrame and keep one of the indices as a third column?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdaf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'series', 'dataframe']\n",
    "    __TITLE = \"Combining two Series into a DataFrame in pandas\"\n",
    "    __QUESTION = \"<p>I have two Series <code>s1</code> and <code>s2</code> with the same (non-consecutive) indices. How do I combine <code>s1</code> and <code>s2</code> to being two columns in a DataFrame and keep one of the indices as a third column?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/18062135/combining-two-series-into-a-dataframe-in-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f7f09",
   "metadata": {},
   "source": [
    "### 92: pandas get rows which are NOT in other dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690f021",
   "metadata": {},
   "source": [
    "<p>Ive two pandas data frames that have some rows in common.</p><p>Suppose dataframe2 is a subset of dataframe1.</p><p><strong>How can I get the rows of dataframe1 which are not in dataframe2?</strong></p><pre><code>df1 = pandas.DataFrame(data = {col1 : [1, 2, 3, 4, 5], col2 : [10, 11, 12, 13, 14]}) df2 = pandas.DataFrame(data = {col1 : [1, 2, 3], col2 : [10, 11, 12]})</code></pre><p>df1</p><pre><code>   col1  col20     1    101     2    112     3    123     4    134     5    14</code></pre><p>df2</p><pre><code>   col1  col20     1    101     2    112     3    12</code></pre><p>Expected result:</p><pre><code>   col1  col23     4    134     5    14</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe']\n",
    "    __TITLE = \"pandas get rows which are NOT in other dataframe\"\n",
    "    __QUESTION = \"<p>Ive two pandas data frames that have some rows in common.</p><p>Suppose dataframe2 is a subset of dataframe1.</p><p><strong>How can I get the rows of dataframe1 which are not in dataframe2?</strong></p><pre><code>df1 = pandas.DataFrame(data = {col1 : [1, 2, 3, 4, 5], col2 : [10, 11, 12, 13, 14]}) df2 = pandas.DataFrame(data = {col1 : [1, 2, 3], col2 : [10, 11, 12]})</code></pre><p>df1</p><pre><code>   col1  col20     1    101     2    112     3    123     4    134     5    14</code></pre><p>df2</p><pre><code>   col1  col20     1    101     2    112     3    12</code></pre><p>Expected result:</p><pre><code>   col1  col23     4    134     5    14</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/28901683/pandas-get-rows-which-are-not-in-other-dataframe\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319b362",
   "metadata": {},
   "source": [
    "### 93: Get column index from column name in python pandas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ca933",
   "metadata": {},
   "source": [
    "<p>In R when you need to retrieve a column index based on the name of the column you could do</p><pre><code>idx &lt;- which(names(my_data)==my_colum_name)</code></pre><p>Is there a way to do the same with pandas dataframes?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'indexing']\n",
    "    __TITLE = \"Get column index from column name in python pandas\"\n",
    "    __QUESTION = \"<p>In R when you need to retrieve a column index based on the name of the column you could do</p><pre><code>idx &lt;- which(names(my_data)==my_colum_name)</code></pre><p>Is there a way to do the same with pandas dataframes?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b77bb",
   "metadata": {},
   "source": [
    "### 94: Apply multiple functions to multiple groupby columns\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9c4e7",
   "metadata": {},
   "source": [
    "<p>The <a href=http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once rel=noreferrer>docs</a> show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:</p><pre><code>In [563]: grouped[D].agg({result1 : np.sum,   .....:                   result2 : np.mean})   .....:Out[563]:       result2   result1A                      bar -0.579846 -1.739537foo -0.280588 -1.402938</code></pre><p>However, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.</p><p>What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, <em>some functions will depend on other columns in the groupby object</em> (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). Ill have to change it so that I iterate through the whole groupby object in a single run, but Im wondering if theres a built in way in pandas to do this somewhat cleanly.</p><p>For example, Ive tried something like</p><pre><code>grouped.agg({C_sum : lambda x: x[C].sum(),             C_std: lambda x: x[C].std(),             D_sum : lambda x: x[D].sum()},             D_sumifC3: lambda x: x[D][x[C] == 3].sum(), ...)</code></pre><p>but as expected I get a KeyError (since the keys have to be a column if <code>agg</code> is called from a DataFrame).</p><p>Is there any built in way to do what Id like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89525221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'group-by', 'aggregate-functions', 'pandas']\n",
    "    __TITLE = \"Apply multiple functions to multiple groupby columns\"\n",
    "    __QUESTION = \"<p>The <a href=http://pandas.pydata.org/pandas-docs/dev/groupby.html#applying-multiple-functions-at-once rel=noreferrer>docs</a> show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:</p><pre><code>In [563]: grouped[D].agg({result1 : np.sum,   .....:                   result2 : np.mean})   .....:Out[563]:       result2   result1A                      bar -0.579846 -1.739537foo -0.280588 -1.402938</code></pre><p>However, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.</p><p>What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, <em>some functions will depend on other columns in the groupby object</em> (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). Ill have to change it so that I iterate through the whole groupby object in a single run, but Im wondering if theres a built in way in pandas to do this somewhat cleanly.</p><p>For example, Ive tried something like</p><pre><code>grouped.agg({C_sum : lambda x: x[C].sum(),             C_std: lambda x: x[C].std(),             D_sum : lambda x: x[D].sum()},             D_sumifC3: lambda x: x[D][x[C] == 3].sum(), ...)</code></pre><p>but as expected I get a KeyError (since the keys have to be a column if <code>agg</code> is called from a DataFrame).</p><p>Is there any built in way to do what Id like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad969ae",
   "metadata": {},
   "source": [
    "### 95: Pandas: drop a level from a multi-level column index?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f3f67",
   "metadata": {},
   "source": [
    "<p>If Ive got a multi-level column index:</p><pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(a, b), (a, c)])&gt;&gt;&gt; pd.DataFrame([[1,2], [3,4]], columns=cols)</code></pre><pre>    a   ---+--    b | c--+---+--0 | 1 | 21 | 3 | 4</pre><p>How can I drop the a level of that index, so I end up with:</p><pre>    b | c--+---+--0 | 1 | 21 | 3 | 4</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"Pandas: drop a level from a multi-level column index?\"\n",
    "    __QUESTION = \"<p>If Ive got a multi-level column index:</p><pre><code>&gt;&gt;&gt; cols = pd.MultiIndex.from_tuples([(a, b), (a, c)])&gt;&gt;&gt; pd.DataFrame([[1,2], [3,4]], columns=cols)</code></pre><pre>    a   ---+--    b | c--+---+--0 | 1 | 21 | 3 | 4</pre><p>How can I drop the a level of that index, so I end up with:</p><pre>    b | c--+---+--0 | 1 | 21 | 3 | 4</pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/22233488/pandas-drop-a-level-from-a-multi-level-column-index\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6d356",
   "metadata": {},
   "source": [
    "### 96: Pandas read in table without headers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dec64e",
   "metadata": {},
   "source": [
    "<p>How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do <code>usecols</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3962b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas']\n",
    "    __TITLE = \"Pandas read in table without headers\"\n",
    "    __QUESTION = \"<p>How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do <code>usecols</code></p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/29287224/pandas-read-in-table-without-headers\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0e3cd",
   "metadata": {},
   "source": [
    "### 97: Extracting just Month and Year separately from Pandas Datetime column\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99bedb",
   "metadata": {},
   "source": [
    "<p>I have a Dataframe, df, with the following column:</p><pre><code>df[ArrivalDate] =...936   2012-12-31938   2012-12-29965   2012-12-31966   2012-12-31967   2012-12-31968   2012-12-31969   2012-12-31970   2012-12-29971   2012-12-31972   2012-12-29973   2012-12-29...</code></pre><p>The elements of the column are pandas.tslib.Timestamp.</p><p>I want to just include the year and month.  I thought there would be simple way to do it, but I cant figure it out.</p><p>Heres what Ive tried:</p><pre><code>df[ArrivalDate].resample(M, how = mean)</code></pre><p>I got the following error:</p><pre><code>Only valid with DatetimeIndex or PeriodIndex </code></pre><p>Then I tried:</p><pre><code>df[ArrivalDate].apply(lambda(x):x[:-2])</code></pre><p>I got the following error:</p><pre><code>Timestamp object has no attribute __getitem__ </code></pre><p>Any suggestions?</p><p>Edit: I sort of figured it out.  </p><pre><code>df.index = df[ArrivalDate]</code></pre><p>Then, I can resample another column using the index.</p><p>But Id still like a method for reconfiguring the entire column.  Any ideas?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'datetime']\n",
    "    __TITLE = \"Extracting just Month and Year separately from Pandas Datetime column\"\n",
    "    __QUESTION = \"<p>I have a Dataframe, df, with the following column:</p><pre><code>df[ArrivalDate] =...936   2012-12-31938   2012-12-29965   2012-12-31966   2012-12-31967   2012-12-31968   2012-12-31969   2012-12-31970   2012-12-29971   2012-12-31972   2012-12-29973   2012-12-29...</code></pre><p>The elements of the column are pandas.tslib.Timestamp.</p><p>I want to just include the year and month.  I thought there would be simple way to do it, but I cant figure it out.</p><p>Heres what Ive tried:</p><pre><code>df[ArrivalDate].resample(M, how = mean)</code></pre><p>I got the following error:</p><pre><code>Only valid with DatetimeIndex or PeriodIndex </code></pre><p>Then I tried:</p><pre><code>df[ArrivalDate].apply(lambda(x):x[:-2])</code></pre><p>I got the following error:</p><pre><code>Timestamp object has no attribute __getitem__ </code></pre><p>Any suggestions?</p><p>Edit: I sort of figured it out.  </p><pre><code>df.index = df[ArrivalDate]</code></pre><p>Then, I can resample another column using the index.</p><p>But Id still like a method for reconfiguring the entire column.  Any ideas?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f61053",
   "metadata": {},
   "source": [
    "### 98: Keep only date part when using pandas.to_datetime\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b7a9e",
   "metadata": {},
   "source": [
    "<p>I use <code>pandas.to_datetime</code> to parse the dates in my data. Pandas by default represents the dates with <code>datetime64[ns]</code> even though the dates are all daily only.I wonder whether there is an elegant/clever way to convert the dates to <code>datetime.date</code> or <code>datetime64[D]</code> so that, when I write the data to CSV, the dates are not appended with <code>00:00:00</code>. I know I can convert the type manually element-by-element:</p><pre><code>[dt.to_datetime().date() for dt in df.dates]</code></pre><p>But this is really slow since I have many rows and it sort of defeats the purpose of using <code>pandas.to_datetime</code>. Is there a way to convert the <code>dtype</code> of the entire column at once? Or alternatively, does <code>pandas.to_datetime</code> support a precision specification so that I can get rid of the time part while working with daily data?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690eb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'csv', 'datetime', 'series']\n",
    "    __TITLE = \"Keep only date part when using pandas.to_datetime\"\n",
    "    __QUESTION = \"<p>I use <code>pandas.to_datetime</code> to parse the dates in my data. Pandas by default represents the dates with <code>datetime64[ns]</code> even though the dates are all daily only.I wonder whether there is an elegant/clever way to convert the dates to <code>datetime.date</code> or <code>datetime64[D]</code> so that, when I write the data to CSV, the dates are not appended with <code>00:00:00</code>. I know I can convert the type manually element-by-element:</p><pre><code>[dt.to_datetime().date() for dt in df.dates]</code></pre><p>But this is really slow since I have many rows and it sort of defeats the purpose of using <code>pandas.to_datetime</code>. Is there a way to convert the <code>dtype</code> of the entire column at once? Or alternatively, does <code>pandas.to_datetime</code> support a precision specification so that I can get rid of the time part while working with daily data?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/16176996/keep-only-date-part-when-using-pandas-to-datetime\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc2292",
   "metadata": {},
   "source": [
    "### 99: How do I Pandas group-by to get sum?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6666d8",
   "metadata": {},
   "source": [
    "<p>I am using this data frame:</p><pre><code>Fruit   Date      Name  NumberApples  10/6/2016 Bob    7Apples  10/6/2016 Bob    8Apples  10/6/2016 Mike   9Apples  10/7/2016 Steve 10Apples  10/7/2016 Bob    1Oranges 10/7/2016 Bob    2Oranges 10/6/2016 Tom   15Oranges 10/6/2016 Mike  57Oranges 10/6/2016 Bob   65Oranges 10/7/2016 Tony   1Grapes  10/7/2016 Bob    1Grapes  10/7/2016 Tom   87Grapes  10/7/2016 Bob   22Grapes  10/7/2016 Bob   12Grapes  10/7/2016 Tony  15</code></pre><p>I want to aggregate this by <code>Name</code> and then by <code>Fruit</code> to get a total number of <code>Fruit</code> per <code>Name</code>. For example:</p><pre><code>Bob,Apples,16</code></pre><p>I tried grouping by <code>Name</code> and <code>Fruit</code> but how do I get the total number of Fruit?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'group-by', 'aggregate']\n",
    "    __TITLE = \"How do I Pandas group-by to get sum?\"\n",
    "    __QUESTION = \"<p>I am using this data frame:</p><pre><code>Fruit   Date      Name  NumberApples  10/6/2016 Bob    7Apples  10/6/2016 Bob    8Apples  10/6/2016 Mike   9Apples  10/7/2016 Steve 10Apples  10/7/2016 Bob    1Oranges 10/7/2016 Bob    2Oranges 10/6/2016 Tom   15Oranges 10/6/2016 Mike  57Oranges 10/6/2016 Bob   65Oranges 10/7/2016 Tony   1Grapes  10/7/2016 Bob    1Grapes  10/7/2016 Tom   87Grapes  10/7/2016 Bob   22Grapes  10/7/2016 Bob   12Grapes  10/7/2016 Tony  15</code></pre><p>I want to aggregate this by <code>Name</code> and then by <code>Fruit</code> to get a total number of <code>Fruit</code> per <code>Name</code>. For example:</p><pre><code>Bob,Apples,16</code></pre><p>I tried grouping by <code>Name</code> and <code>Fruit</code> but how do I get the total number of Fruit?</p>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/39922986/how-do-i-pandas-group-by-to-get-sum\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d48b",
   "metadata": {},
   "source": [
    "### 100: Pandas Replace NaN with blank/empty string\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745764a",
   "metadata": {},
   "source": [
    "<p>I have a Pandas Dataframe as shown below:</p><pre><code>    1    2       3 0  a  NaN    read 1  b    l  unread 2  c  NaN    read</code></pre><p>I want to remove the NaN values with an empty string so that it looks like so:</p><pre><code>    1    2       3 0  a       read 1  b    l  unread 2  c       read</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8470c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    # Please do not modify these unless it is very intentional\n",
    "    __TAGS = ['python', 'pandas', 'dataframe', 'nan']\n",
    "    __TITLE = \"Pandas Replace NaN with blank/empty string\"\n",
    "    __QUESTION = \"<p>I have a Pandas Dataframe as shown below:</p><pre><code>    1    2       3 0  a  NaN    read 1  b    l  unread 2  c  NaN    read</code></pre><p>I want to remove the NaN values with an empty string so that it looks like so:</p><pre><code>    1    2       3 0  a       read 1  b    l  unread 2  c       read</code></pre>\"\n",
    "    __ORIGINAL_LINK = \"https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string\"\n",
    "\n",
    "    def answer():\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # optional markdown solution for easier reading\n",
    "    MARKDOWN_SOLUTION = \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    Post.answer()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
